{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cbdc6cd",
   "metadata": {},
   "source": [
    "based on main.py\r\n",
    "\r\n",
    "runs Continuous Query Decomposition\r\n",
    "\r\n",
    "ref: https://github.com/pminervini/KGReaso\n",
    "\n",
    "changed self.embeddings = nn.ModuleList([nn.Embedding(s, 2 * rank, sparse=False) for s in sizes[:2]]) sparse to False in cqd/base.pyning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4ab19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\r\n",
    "from __future__ import division\r\n",
    "from __future__ import print_function\r\n",
    "\r\n",
    "import argparse\r\n",
    "import json\r\n",
    "import logging\r\n",
    "import os\r\n",
    "import random\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from models import KGReasoning\r\n",
    "# SingledirectionalOneShotIterator provides negative sampling\r\n",
    "from dataloader import TestDataset, TrainDataset, SingledirectionalOneShotIterator\r\n",
    "# from tensorboardX import SummaryWriter\r\n",
    "import time\r\n",
    "import pickle\r\n",
    "from collections import defaultdict\r\n",
    "from tqdm import tqdm\r\n",
    "from util import flatten_query, list2tuple, parse_time, set_global_seed, eval_tuple\r\n",
    "\r\n",
    "import collections\r\n",
    "import random\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "from cqd import CQD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab773349",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a32189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', 'up-DNF', '2u-DM', 'up-DM']\n"
     ]
    }
   ],
   "source": [
    "query_name_dict = {('e',('r',)): '1p', \r\n",
    "                    ('e', ('r', 'r')): '2p',\r\n",
    "                    ('e', ('r', 'r', 'r')): '3p',\r\n",
    "                    (('e', ('r',)), ('e', ('r',))): '2i',\r\n",
    "                    (('e', ('r',)), ('e', ('r',)), ('e', ('r',))): '3i',\r\n",
    "                    ((('e', ('r',)), ('e', ('r',))), ('r',)): 'ip',\r\n",
    "                    (('e', ('r', 'r')), ('e', ('r',))): 'pi',\r\n",
    "                    (('e', ('r',)), ('e', ('r', 'n'))): '2in',\r\n",
    "                    (('e', ('r',)), ('e', ('r',)), ('e', ('r', 'n'))): '3in',\r\n",
    "                    ((('e', ('r',)), ('e', ('r', 'n'))), ('r',)): 'inp',\r\n",
    "                    (('e', ('r', 'r')), ('e', ('r', 'n'))): 'pin',\r\n",
    "                    (('e', ('r', 'r', 'n')), ('e', ('r',))): 'pni',\r\n",
    "                    (('e', ('r',)), ('e', ('r',)), ('u',)): '2u-DNF',\r\n",
    "                    ((('e', ('r',)), ('e', ('r',)), ('u',)), ('r',)): 'up-DNF',\r\n",
    "                    ((('e', ('r', 'n')), ('e', ('r', 'n'))), ('n',)): '2u-DM',\r\n",
    "                    ((('e', ('r', 'n')), ('e', ('r', 'n'))), ('n', 'r')): 'up-DM'\r\n",
    "                }\r\n",
    "name_query_dict = {value: key for key, value in query_name_dict.items()}\r\n",
    "all_tasks = list(name_query_dict.keys()) # ['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', '2u-DM', 'up-DNF', 'up-DM']\r\n",
    "\r\n",
    "print(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f295a6-39c2-4acd-97ac-beaae2a0ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyArgs:\n",
    "    def __init__(self):\n",
    "        None\n",
    "        \n",
    "args = DummyArgs()\n",
    "args.cuda = True\n",
    "args.geo = \"cqd\" # choices=['vec', 'box', 'beta', 'cqd']\n",
    "args.gamma = 12.0\n",
    "args.box_mode = \"(none,0.02)\" # Query2box\n",
    "args.beta_mode = \"(1600,2)\" # BetaE relational projection\n",
    "args.cqd_type = \"discrete\" # Continuous Query Decomposition\n",
    "args.cqd_t_norm = CQD.PROD_NORM\n",
    "args.cqd_k = 5\n",
    "args.cqd_sigmoid_scores = False\n",
    "args.cqd_normalize_scores = False\n",
    "args.reg_weight = 1e-3\n",
    "args.use_qa_iterator = True\n",
    "args.data_path = \"data/FB15k-237-betae\"\n",
    "args.batch_size = 64 #1024\n",
    "args.cpu_num = 1# 10\n",
    "args.negative_sample_size = 128\n",
    "args.hidden_dim = 500\n",
    "args.test_batch_size = 3 #2\n",
    "args.print_on_screen = True\n",
    "args.test_log_steps = 1000\n",
    "args.learning_rate = 0.0001\n",
    "args.max_steps = 1000 #100000\n",
    "args.evaluate_union = \"DNF\" # choices=['DNF', 'DM'] evaluate union querioes, disjunctive normal form (DNF) or de Morgan's laws (DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2655485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', 'up-DNF', '2u-DM', 'up-DM']\n"
     ]
    }
   ],
   "source": [
    "# different query types\r\n",
    "tasks = all_tasks[0:1]\r\n",
    "\r\n",
    "if args.geo in ['box']:\r\n",
    "    tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.box_mode)\r\n",
    "elif args.geo in ['vec']:\r\n",
    "    tmp_str = \"g-{}\".format(args.gamma)\r\n",
    "elif args.geo == 'beta':\r\n",
    "    tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.beta_mode)\r\n",
    "elif args.geo == 'cqd':\r\n",
    "    tmp_str = \"g-cqd\"\r\n",
    "\r\n",
    "print(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6020bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/stats.txt' % args.data_path) as f:\r\n",
    "    entrel = f.readlines()\r\n",
    "    nentity = int(entrel[0].split(' ')[-1])\r\n",
    "    nrelation = int(entrel[1].split(' ')[-1])\r\n",
    "\r\n",
    "args.nentity = nentity\r\n",
    "args.nrelation = nrelation\r\n",
    "\r\n",
    "logging.info('-------------------------------'*3)\r\n",
    "logging.info('Geo: %s' % args.geo)\r\n",
    "logging.info('Data Path: %s' % args.data_path)\r\n",
    "logging.info('#entity: %d' % nentity)\r\n",
    "logging.info('#relation: %d' % nrelation)\r\n",
    "# logging.info('#max steps: %d' % args.max_steps)\r\n",
    "# logging.info('Evaluate unoins using: %s' % args.evaluate_union)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd982816",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f0e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, tasks):\r\n",
    "    '''\r\n",
    "    Load queries and remove queries not in tasks\r\n",
    "    '''\r\n",
    "    logging.info(\"loading data\")\r\n",
    "    train_queries = pickle.load(open(os.path.join(args.data_path, \"train-queries.pkl\"), 'rb'))\r\n",
    "    train_answers = pickle.load(open(os.path.join(args.data_path, \"train-answers.pkl\"), 'rb'))\r\n",
    "    valid_queries = pickle.load(open(os.path.join(args.data_path, \"valid-queries.pkl\"), 'rb'))\r\n",
    "    valid_hard_answers = pickle.load(open(os.path.join(args.data_path, \"valid-hard-answers.pkl\"), 'rb'))\r\n",
    "    valid_easy_answers = pickle.load(open(os.path.join(args.data_path, \"valid-easy-answers.pkl\"), 'rb'))\r\n",
    "    test_queries = pickle.load(open(os.path.join(args.data_path, \"test-queries.pkl\"), 'rb'))\r\n",
    "    test_hard_answers = pickle.load(open(os.path.join(args.data_path, \"test-hard-answers.pkl\"), 'rb'))\r\n",
    "    test_easy_answers = pickle.load(open(os.path.join(args.data_path, \"test-easy-answers.pkl\"), 'rb'))\r\n",
    "    \r\n",
    "    # remove tasks not in args.tasks\r\n",
    "    for name in all_tasks:\r\n",
    "        if 'u' in name:\r\n",
    "            name, evaluate_union = name.split('-')\r\n",
    "        else:\r\n",
    "            evaluate_union = args.evaluate_union\r\n",
    "        if name not in tasks or evaluate_union != args.evaluate_union:\r\n",
    "            query_structure = name_query_dict[name if 'u' not in name else '-'.join([name, evaluate_union])]\r\n",
    "            if query_structure in train_queries:\r\n",
    "                del train_queries[query_structure]\r\n",
    "            if query_structure in valid_queries:\r\n",
    "                del valid_queries[query_structure]\r\n",
    "            if query_structure in test_queries:\r\n",
    "                del test_queries[query_structure]\r\n",
    "\r\n",
    "    return train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d1f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\r\n",
    "LOAD DATA\r\n",
    "based on tasks defined in args\r\n",
    "\"\"\"\r\n",
    "train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers = load_data(args, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a12ff8b-e6ad-4d56-ba1e-8e8402359240",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspect dataset\n",
    "\n",
    "# len(test_queries[('e', ('r',))]) #, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers\n",
    "# valid_hard_answers.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3e50b",
   "metadata": {},
   "source": [
    "1. train_queries: for (e, r) given entityID and relationID, find all other entities that are connected to it.\n",
    "2. train_answers: given (e, r) this is the groundtruth answers of all entityIDs that are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11a197a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(collections.defaultdict, 1),\n",
       " (collections.defaultdict, 1496890),\n",
       " (collections.defaultdict, 1),\n",
       " (collections.defaultdict, 95094),\n",
       " (collections.defaultdict, 95094),\n",
       " (collections.defaultdict, 1),\n",
       " (collections.defaultdict, 97804),\n",
       " (collections.defaultdict, 97804)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(type(x), len(x)) for x in [train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19f9ff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "(6288, (8,))\n",
      "False\n",
      "valid_hard_answers set()\n",
      "valid_easy_answers set()\n",
      "**********\n",
      "(6288, (8,))\n",
      "True\n",
      "train_answers {9, 11, 117, 399}\n",
      "**********\n",
      "(6288, (8,))\n",
      "False\n",
      "test_hard_answers set()\n",
      "test_easy_answers set()\n"
     ]
    }
   ],
   "source": [
    "ex_val_q = (6288, (8,))\r\n",
    "print(\"*\" * 10)\r\n",
    "print(ex_val_q)\r\n",
    "print(ex_val_q in valid_queries[('e', ('r', ))])\r\n",
    "print(\"valid_hard_answers\", valid_hard_answers[ex_val_q])\r\n",
    "print(\"valid_easy_answers\", valid_easy_answers[ex_val_q])\r\n",
    "\r\n",
    "# get example query\r\n",
    "ex_q = ex_val_q #(6288, (8,))\r\n",
    "print(\"*\" * 10)\r\n",
    "print(ex_q)\r\n",
    "print(ex_q in train_queries[('e', ('r', ))])\r\n",
    "print(\"train_answers\", train_answers[ex_q])\r\n",
    "\r\n",
    "ex_test_q = ex_val_q\r\n",
    "print(\"*\" * 10)\r\n",
    "print(ex_test_q)\r\n",
    "print(ex_test_q in test_queries[('e', ('r', ))])\r\n",
    "print(\"test_hard_answers\", test_hard_answers[ex_val_q])\r\n",
    "print(\"test_easy_answers\", test_easy_answers[ex_val_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f2fb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train_queries 149689\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "TRAINING Data Preparation\r\n",
    "\"\"\"\r\n",
    "train_path_queries = defaultdict(set)\r\n",
    "for query_structure in train_queries:\r\n",
    "#     train_path_queries[query_structure] = set(random.sample(train_queries[query_structure], 1))\r\n",
    "    train_path_queries[query_structure] = train_queries[query_structure]\r\n",
    "            \r\n",
    "train_path_queries = flatten_query(train_path_queries)\r\n",
    "print(\"number of train_queries\", len(train_path_queries))\r\n",
    "train_path_iterator = SingledirectionalOneShotIterator(DataLoader(\r\n",
    "                            # {(e,r): (entityID, relID)} and {(e, r): [entityID]}\r\n",
    "                            TrainDataset(train_path_queries, nentity, nrelation, args.negative_sample_size, train_answers),\r\n",
    "                            batch_size=args.batch_size,\r\n",
    "                            shuffle=True,\r\n",
    "                            num_workers=args.cpu_num,\r\n",
    "                            collate_fn=TrainDataset.collate_fn\r\n",
    "                        ))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba4883a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error ('e', ('r',))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "VALIDATION Data Preparation\r\n",
    "\"\"\"\r\n",
    "# for each type of structure / key\r\n",
    "for query_structure in valid_queries:\r\n",
    "    try:\r\n",
    "        print(query_name_dict[query_structure[1]]+\": \"+str(len(valid_queries[query_structure])))\r\n",
    "    except:\r\n",
    "        print(\"error\", query_structure)\r\n",
    "\r\n",
    "valid_dataloader = DataLoader(\r\n",
    "    TestDataset(\r\n",
    "        flatten_query(valid_queries), \r\n",
    "        args.nentity, \r\n",
    "        args.nrelation, \r\n",
    "    ), \r\n",
    "    batch_size=args.test_batch_size,\r\n",
    "    num_workers=args.cpu_num, \r\n",
    "    collate_fn=TestDataset.collate_fn\r\n",
    ")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad1da77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter embeddings.0.weight: torch.Size([14505, 1000]), require_grad = True\n",
      "Parameter embeddings.1.weight: torch.Size([474, 1000]), require_grad = True\n",
      "Parameter Number: 14979000\n"
     ]
    }
   ],
   "source": [
    "model = CQD(nentity,\n",
    "    nrelation,\n",
    "    rank=args.hidden_dim,\n",
    "    test_batch_size=args.test_batch_size,\n",
    "    reg_weight=args.reg_weight,\n",
    "    query_name_dict=query_name_dict,\n",
    "    method=args.cqd_type,\n",
    "    t_norm_name=args.cqd_t_norm,\n",
    "    k=args.cqd_k,\n",
    "    do_sigmoid=args.cqd_sigmoid_scores,\n",
    "    do_normalize=args.cqd_normalize_scores,\n",
    "    use_cuda=args.cuda)\n",
    "\n",
    "logging.info('Model Parameter Configuration:')\n",
    "num_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n",
    "    if param.requires_grad:\n",
    "        num_params += np.prod(param.size())\n",
    "print('Parameter Number: %d' % num_params)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "current_learning_rate = args.learning_rate\n",
    "optimizer = torch.optim.Adam(\n",
    "# optimizer = torch.optim.SparseAdam(\n",
    "# optimizer = torch.optim.RMSprop(\n",
    "# optimizer = torch.optim.Adagrad(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=current_learning_rate\n",
    ")\n",
    "warm_up_steps = args.max_steps // 2\n",
    "_steps = args.max_steps // 2\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff77513",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b945573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: eekosasih (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">2021-09-13T14:08:25.033666</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/eekosasih/KGReasoning\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/eekosasih/KGReasoning/runs/2i93xxda\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning/runs/2i93xxda</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210913_150826-2i93xxda</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2i93xxda)</h1><iframe src=\"https://wandb.ai/eekosasih/KGReasoning/runs/2i93xxda\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29958deb888>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import datetime\n",
    "\n",
    "timenow = datetime.datetime.utcnow().isoformat()\n",
    "\n",
    "wandb.init(project=\"KGReasoning\", name=timenow, group=\"cqd\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b26e82df-e254-438b-82b0-a2989540e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c6448a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-231652856dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# train model for a step over all batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKGReasoning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_path_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtraining_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\KGReasoning\\models.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, optimizer, train_iterator, args, step)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         log = {\n\u001b[1;32m--> 872\u001b[1;33m             \u001b[1;34m'positive_sample_loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpositive_sample_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m             \u001b[1;34m'negative_sample_loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnegative_sample_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[1;34m'loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "TRAIN LOOP\r\n",
    "\"\"\"\r\n",
    "init_step = 0\r\n",
    "\r\n",
    "training_logs = []\r\n",
    "# loop over all batches\r\n",
    "# initialising train_path_iterator is expensive. So at certain loop it might take some time to load\r\n",
    "for step in tqdm(range(init_step, args.max_steps)):\r\n",
    "    # train model for a step over all batches\r\n",
    "    log = KGReasoning.train_step(model, optimizer, train_path_iterator, args, step)\r\n",
    "    training_logs.append(log)\r\n",
    "\r\n",
    "    # raise Exception(\"\")\r\n",
    "\r\n",
    "    log[\"epoch\"] = step\r\n",
    "    wandb.log(log, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5858a0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29504<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210913_150826-2i93xxda\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210913_150826-2i93xxda\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>positive_sample_loss</td><td>16.42752</td></tr><tr><td>negative_sample_loss</td><td>16.42752</td></tr><tr><td>loss</td><td>16.42752</td></tr><tr><td>epoch</td><td>6778</td></tr><tr><td>_runtime</td><td>743</td></tr><tr><td>_timestamp</td><td>1631542849</td></tr><tr><td>_step</td><td>6778</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>positive_sample_loss</td><td>████████▇▇▇▇▇▇▆▇▆▇▆▅▆▆▄▅▄▄▅▄▄▄▃▃▂▂▂▃▁▃▁▁</td></tr><tr><td>negative_sample_loss</td><td>████████▇▇▇▇▇▇▆▇▆▇▆▅▆▆▄▅▄▄▅▄▄▄▃▃▂▂▂▃▁▃▁▁</td></tr><tr><td>loss</td><td>████████▇▇▇▇▇▇▆▇▆▇▆▅▆▆▄▅▄▄▅▄▄▄▃▃▂▂▂▃▁▃▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">2021-09-13T14:08:25.033666</strong>: <a href=\"https://wandb.ai/eekosasih/KGReasoning/runs/2i93xxda\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning/runs/2i93xxda</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74480093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28e6fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\r\n",
    "torch.save(\r\n",
    "    {\r\n",
    "        'model_state_dict': model.state_dict(),\r\n",
    "        'optimizer_state_dict': optimizer.state_dict()\r\n",
    "    },\r\n",
    "    os.path.join(\"checkpoint\", \"model.pt\")\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157bd695",
   "metadata": {},
   "source": [
    "## DEBUG TRAINING STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "975e5213-b579-4570-b98a-7629283fa1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "model, optimizer, train_iterator, args, step = model, optimizer, train_path_iterator, args, step\n",
    "\n",
    "# there's an overhead operation to shufflte the train_iterator that makes this expensive to run\n",
    "positive_sample, negative_sample, subsampling_weight, batch_queries, query_structures = next(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61faccad-7e8e-4b39-bd12-dbc8be093a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive_sample_loss': 19.164501190185547,\n",
       " 'negative_sample_loss': 19.164501190185547,\n",
       " 'loss': 19.164501190185547}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step = 0\n",
    "\n",
    "# KGReasoning.train_step(model, optimizer, train_other_iterator, args, step)\n",
    "\n",
    "model, optimizer, train_iterator, args, step = model, optimizer, train_path_iterator, args, step\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# # there's an overhead operation to shufflte the train_iterator that makes this expensive to run\n",
    "# positive_sample, negative_sample, subsampling_weight, batch_queries, query_structures = next(train_iterator)\n",
    "\n",
    "# group queries into batch\n",
    "batch_queries_dict = collections.defaultdict(list)\n",
    "batch_idxs_dict = collections.defaultdict(list)\n",
    "for i, query in enumerate(batch_queries): # group queries with same structure\n",
    "    batch_queries_dict[query_structures[i]].append(query)\n",
    "    batch_idxs_dict[query_structures[i]].append(i)\n",
    "\n",
    "for query_structure in batch_queries_dict:\n",
    "    if args.cuda:\n",
    "        batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure]).cuda()\n",
    "    else:\n",
    "        batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure])\n",
    "\n",
    "if args.cuda:\n",
    "    positive_sample = positive_sample.cuda()\n",
    "    negative_sample = negative_sample.cuda()\n",
    "    subsampling_weight = subsampling_weight.cuda()\n",
    "\n",
    "if isinstance(model, CQD):\n",
    "    input_batch = batch_queries_dict[('e', ('r',))]\n",
    "    input_batch = torch.cat((input_batch, positive_sample.unsqueeze(1)), dim=1)\n",
    "    \n",
    "    model = model.cpu()\n",
    "    input_batch = input_batch.cpu()\n",
    "    \n",
    "    loss = model.loss(input_batch)\n",
    "\n",
    "    positive_sample_loss = negative_sample_loss = loss\n",
    "\n",
    "    \n",
    "# # score positive and negative samples\n",
    "# positive_logit, negative_logit, subsampling_weight, _ = model(positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict)\n",
    "\n",
    "# # calculate loss\n",
    "# # positive samples should have label of 1 while negative samples label of 0\n",
    "# negative_score = F.logsigmoid(-negative_logit).mean(dim=1)\n",
    "# positive_score = F.logsigmoid(positive_logit).mean(dim=1)\n",
    "# # aggregate loss with subsampling_weight\n",
    "# positive_sample_loss = - (subsampling_weight * positive_score).sum()\n",
    "# negative_sample_loss = - (subsampling_weight * negative_score).sum()\n",
    "# positive_sample_loss /= subsampling_weight.sum()\n",
    "# negative_sample_loss /= subsampling_weight.sum()\n",
    "# loss = (positive_sample_loss + negative_sample_loss)/2\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "log = {\n",
    "    'positive_sample_loss': positive_sample_loss.item(),\n",
    "    'negative_sample_loss': negative_sample_loss.item(),\n",
    "    'loss': loss.item(),\n",
    "}\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0117e6a4-3353-4dd2-83b2-92eca4ac5baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.1645, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" track loss = model.loss(input_batch) \"\"\"\n",
    "triples = input_batch\n",
    "(scores_o, scores_s), factors = model.score_candidates(triples)\n",
    "l_fit = model.loss_fn(scores_o, triples[:, 2]) + model.loss_fn(scores_s, triples[:, 0])\n",
    "l_reg = model.regularizer.forward(factors)\n",
    "\n",
    "# l_fit.backward()\n",
    "l_reg.backward()\n",
    "optimizer.step()\n",
    "\n",
    "l_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "527eb7d5-40c6-4181-aef7-0d22c5ea976f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CQD(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(14505, 1000, sparse=True)\n",
       "    (1): Embedding(474, 1000, sparse=True)\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b9bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== input ===\n",
      "torch.Size([64]) torch.Size([64, 128]) 64 64 torch.Size([64])\n",
      "=== output ===\n",
      "torch.Size([64, 1]) torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== input ===\")\r\n",
    "print(positive_sample.shape, negative_sample.shape, len(batch_queries), len(query_structures), subsampling_weight.shape)\r\n",
    "print(\"=== output ===\")\r\n",
    "print(positive_logit.shape, negative_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7014102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf13cfa4",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2083ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model\r\n",
    "checkpoint = torch.load(os.path.join(\"checkpoint\", \"model.pt\"))\r\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125affbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tp_answers, fn_answers, args, dataloader, query_name_dict, mode, step):\r\n",
    "    '''\r\n",
    "    Evaluate queries in dataloader\r\n",
    "    '''\r\n",
    "    average_metrics = defaultdict(float)\r\n",
    "    all_metrics = defaultdict(float)\r\n",
    "\r\n",
    "    metrics = model.test_step(model, tp_answers, fn_answers, args, dataloader, query_name_dict)\r\n",
    "    num_query_structures = 0\r\n",
    "    num_queries = 0\r\n",
    "    for query_structure in metrics:\r\n",
    "        log_metrics(mode+\" \"+query_name_dict[query_structure], step, metrics[query_structure])\r\n",
    "        for metric in metrics[query_structure]:\r\n",
    "            all_metrics[\"_\".join([query_name_dict[query_structure], metric])] = metrics[query_structure][metric]\r\n",
    "            if metric != 'num_queries':\r\n",
    "                average_metrics[metric] += metrics[query_structure][metric]\r\n",
    "        num_queries += metrics[query_structure]['num_queries']\r\n",
    "        num_query_structures += 1\r\n",
    "\r\n",
    "    for metric in average_metrics:\r\n",
    "        average_metrics[metric] /= num_query_structures\r\n",
    "        all_metrics[\"_\".join([\"average\", metric])] = average_metrics[metric]\r\n",
    "    log_metrics('%s average'%mode, step, average_metrics)\r\n",
    "\r\n",
    "    return all_metrics\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e554acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(mode, step, metrics):\r\n",
    "    '''\r\n",
    "    Print the evaluation logs\r\n",
    "    '''\r\n",
    "    for metric in metrics:\r\n",
    "        logging.info('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b067e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [01:06<00:00, 100.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'1p_MRR': 0.366261465070138,\n",
       "             '1p_HITS1': 0.2592751371505161,\n",
       "             '1p_HITS3': 0.41276127284368985,\n",
       "             '1p_HITS10': 0.5799358917393381,\n",
       "             '1p_num_queries': 20094,\n",
       "             'average_MRR': 0.366261465070138,\n",
       "             'average_HITS1': 0.2592751371505161,\n",
       "             'average_HITS3': 0.41276127284368985,\n",
       "             'average_HITS10': 0.5799358917393381})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"`\r\n",
    "EVAL\r\n",
    "\"\"\"\r\n",
    "step = -1\r\n",
    "valid_all_metrics = evaluate(model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict, 'Valid', step)\r\n",
    "valid_all_metrics\r\n",
    "\r\n",
    "# after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe6a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc004201",
   "metadata": {},
   "source": [
    "## DEBUG TEST STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88ee7bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [01:07<00:00, 99.49it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function models.KGReasoning.test_step.<locals>.<lambda>()>,\n",
       "            {('e', ('r',)): defaultdict(int,\n",
       "                         {'MRR': 0.009518244747182089,\n",
       "                          'HITS1': 0.008909909141400957,\n",
       "                          'HITS3': 0.009125562239005588,\n",
       "                          'HITS10': 0.009491169651853655,\n",
       "                          'num_queries': 20094})})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def evaluate in main.py\"\"\"\r\n",
    "\r\n",
    "metrics = model.test_step(model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict)\r\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13c2e08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38656f364568485e829223a8155cf66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "         [    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "         [    0,     1,     2,  ..., 14502, 14503, 14504]]),\n",
       " [[11217, 143], [5673, 34], [4190, 62]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for negative_sample, queries, queries_unflatten, query_structures in tqdm(valid_dataloader, disable=not args.print_on_screen):\r\n",
    "    break\r\n",
    "    \r\n",
    "negative_sample, queries, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58937bb3",
   "metadata": {},
   "source": [
    "1. why is there no positive_sample in test_dataloader\n",
    "2. how to embed query vector and perform projection & intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dda9374b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "        [    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "        [    0,     1,     2,  ..., 14502, 14503, 14504]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75527edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd9a696432d44d59c60123ec978ca1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {('e', ('r',)): defaultdict(int,\n",
       "                         {'MRR': 0.009518244747182089,\n",
       "                          'HITS1': 0.008909909141400957,\n",
       "                          'HITS3': 0.009125562239005588,\n",
       "                          'HITS10': 0.009491169651853655,\n",
       "                          'num_queries': 20094})})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.test_step in models.py\"\"\"\r\n",
    "model, easy_answers, hard_answers, args, test_dataloader, query_name_dict = model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict\r\n",
    "\r\n",
    "model.eval()\r\n",
    "\r\n",
    "step = 0\r\n",
    "total_steps = len(test_dataloader)\r\n",
    "logs = collections.defaultdict(list)\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    # process per batch\r\n",
    "    # loop through every validation dataset\r\n",
    "    # negative sample is basically all entities in nentity\r\n",
    "    for negative_sample, queries, queries_unflatten, query_structures in tqdm(test_dataloader, disable=not args.print_on_screen):\r\n",
    "        batch_queries_dict = collections.defaultdict(list)\r\n",
    "        batch_idxs_dict = collections.defaultdict(list)\r\n",
    "        \r\n",
    "        # for each test query\r\n",
    "        for i, query in enumerate(queries):\r\n",
    "            batch_queries_dict[query_structures[i]].append(query)\r\n",
    "            batch_idxs_dict[query_structures[i]].append(i)\r\n",
    "            \r\n",
    "        # convert each positive query to a LongTensor\r\n",
    "        for query_structure in batch_queries_dict:\r\n",
    "            if args.cuda:\r\n",
    "                batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure]).cuda()\r\n",
    "            else:\r\n",
    "                batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure])\r\n",
    "            \r\n",
    "        # convert each negative query to a LongTensor\r\n",
    "        if args.cuda:\r\n",
    "            negative_sample = negative_sample.cuda()\r\n",
    "            \r\n",
    "        # get prediction output\r\n",
    "        # (2, number_of_edges) same dimension with negative_sample\r\n",
    "        # this is basically embedding lookup\r\n",
    "        \"\"\"model.forward\"\"\"\r\n",
    "        # negative_sample here is just a list of all entities, some of them are actually positive\r\n",
    "        _, negative_logit, _, idxs = model(None, negative_sample, None, batch_queries_dict, batch_idxs_dict)\r\n",
    "          \r\n",
    "        # ... scoring ...\r\n",
    "        queries_unflatten = [queries_unflatten[i] for i in idxs]\r\n",
    "        query_structures = [query_structures[i] for i in idxs]\r\n",
    "        # sort from maximum value based on logit values\r\n",
    "        argsort = torch.argsort(negative_logit, dim=1, descending=True)\r\n",
    "        ranking = argsort.clone().to(torch.float)\r\n",
    "        if len(argsort) == args.test_batch_size: # if it is the same shape with test_batch_size, we can reuse batch_entity_range without creating a new one\r\n",
    "            ranking = ranking.scatter_(1, argsort, model.batch_entity_range) # achieve the ranking of all entities\r\n",
    "        else: # otherwise, create a new torch Tensor for batch_entity_range\r\n",
    "            if args.cuda:\r\n",
    "                ranking = ranking.scatter_(1, argsort, torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0], 1).cuda()) # achieve the ranking of all entities\r\n",
    "            else:\r\n",
    "                ranking = ranking.scatter_(1, argsort, torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0], 1)) # achieve the ranking of all entities\r\n",
    "        \r\n",
    "        # loop through ranking\r\n",
    "        # score every query in the dataloader batch\r\n",
    "        for idx, (i, query, query_structure) in enumerate(zip(argsort[:, 0], queries_unflatten, query_structures)):\r\n",
    "            # get groundtruth labels\r\n",
    "            hard_answer = hard_answers[query]\r\n",
    "            easy_answer = easy_answers[query]\r\n",
    "            num_hard = len(hard_answer)\r\n",
    "            num_easy = len(easy_answer)\r\n",
    "            assert len(hard_answer.intersection(easy_answer)) == 0\r\n",
    "\r\n",
    "            # compare ranking of groundtruth (easy_answer, hard_answer) and predicted results\r\n",
    "            cur_ranking = ranking[idx, list(easy_answer) + list(hard_answer)]\r\n",
    "            cur_ranking, indices = torch.sort(cur_ranking)\r\n",
    "            masks = indices >= num_easy\r\n",
    "            if args.cuda:\r\n",
    "                answer_list = torch.arange(num_hard + num_easy).to(torch.float).cuda()\r\n",
    "            else:\r\n",
    "                answer_list = torch.arange(num_hard + num_easy).to(torch.float)\r\n",
    "            cur_ranking = cur_ranking - answer_list + 1 # filtered setting\r\n",
    "            cur_ranking = cur_ranking[masks] # only take indices that belong to the hard answers\r\n",
    "\r\n",
    "            mrr = torch.mean(1./cur_ranking).item()\r\n",
    "            h1 = torch.mean((cur_ranking <= 1).to(torch.float)).item()\r\n",
    "            h3 = torch.mean((cur_ranking <= 3).to(torch.float)).item()\r\n",
    "            h10 = torch.mean((cur_ranking <= 10).to(torch.float)).item()\r\n",
    "\r\n",
    "            logs[query_structure].append({\r\n",
    "                'MRR': mrr,\r\n",
    "                'HITS1': h1,\r\n",
    "                'HITS3': h3,\r\n",
    "                'HITS10': h10,\r\n",
    "                'num_hard_answer': num_hard,\r\n",
    "            })\r\n",
    "\r\n",
    "metrics = collections.defaultdict(lambda: collections.defaultdict(int))\r\n",
    "for query_structure in logs:\r\n",
    "    for metric in logs[query_structure][0].keys():\r\n",
    "        if metric in ['num_hard_answer']:\r\n",
    "            continue\r\n",
    "        metrics[query_structure][metric] = sum([log[metric] for log in logs[query_structure]])/len(logs[query_structure])\r\n",
    "    metrics[query_structure]['num_queries'] = len(logs[query_structure])\r\n",
    "\r\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0b3c725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[0.8224, 0.7421, 0.8782,  ..., 0.5703, 0.7527, 0.5376],\n",
       "         [0.5680, 0.4127, 0.2810,  ..., 0.4131, 0.6748, 0.3881],\n",
       "         [0.6553, 1.1643, 1.2482,  ..., 0.8182, 1.0112, 1.0331]],\n",
       "        device='cuda:0', grad_fn=<CatBackward>),\n",
       " None,\n",
       " [0, 1, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.forward\"\"\"\r\n",
    "# model.forward(self, positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict)\r\n",
    "\r\n",
    "# model.geo == \"vec\"\r\n",
    "\r\n",
    "# done per batch\r\n",
    "\"\"\"model.forward_vec\"\"\"\r\n",
    "# model.forward_vec(self, positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict)\r\n",
    "# negative_sample is a list of entities to be scored as potential answers to the queries\r\n",
    "# batch_queries_dict stores list of queries (in this case (e, r) to be scored)\r\n",
    "positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict = None, negative_sample, None, batch_queries_dict, batch_idxs_dict\r\n",
    "\r\n",
    "all_center_embeddings, all_idxs = [], []\r\n",
    "all_union_center_embeddings, all_union_idxs = [], []\r\n",
    "for query_structure in batch_queries_dict:\r\n",
    "    if 'u' in model.query_name_dict[query_structure]:\r\n",
    "        # embedding \r\n",
    "        center_embedding, _ = model.embed_query_vec(model.transform_union_query(batch_queries_dict[query_structure], \r\n",
    "                                                            query_structure), \r\n",
    "                                                        model.transform_union_structure(query_structure), 0)\r\n",
    "        all_union_center_embeddings.append(center_embedding)\r\n",
    "        all_union_idxs.extend(batch_idxs_dict[query_structure])\r\n",
    "    else:\r\n",
    "        # get vector embedding for each query (anchor + projection + intersection)\r\n",
    "        # this will be compared with the groundtruth answer entity embedding\r\n",
    "        center_embedding, _ = model.embed_query_vec(batch_queries_dict[query_structure], query_structure, 0)\r\n",
    "        all_center_embeddings.append(center_embedding)\r\n",
    "        all_idxs.extend(batch_idxs_dict[query_structure])\r\n",
    "\r\n",
    "if len(all_center_embeddings) > 0:\r\n",
    "    all_center_embeddings = torch.cat(all_center_embeddings, dim=0).unsqueeze(1)\r\n",
    "if len(all_union_center_embeddings) > 0:\r\n",
    "    all_union_center_embeddings = torch.cat(all_union_center_embeddings, dim=0).unsqueeze(1)\r\n",
    "    all_union_center_embeddings = all_union_center_embeddings.view(all_union_center_embeddings.shape[0]//2, 2, 1, -1)\r\n",
    "\r\n",
    "if type(subsampling_weight) != type(None):\r\n",
    "    subsampling_weight = subsampling_weight[all_idxs+all_union_idxs]\r\n",
    "\r\n",
    "# in test, positive samples are not given\r\n",
    "if type(positive_sample) != type(None):\r\n",
    "    if len(all_center_embeddings) > 0:\r\n",
    "        positive_sample_regular = positive_sample[all_idxs]\r\n",
    "        positive_embedding = torch.index_select(model.entity_embedding, dim=0, index=positive_sample_regular).unsqueeze(1)\r\n",
    "        positive_logit = model.cal_logit_vec(positive_embedding, all_center_embeddings)\r\n",
    "    else:\r\n",
    "        positive_logit = torch.Tensor([]).to(model.entity_embedding.device)\r\n",
    "\r\n",
    "    if len(all_union_center_embeddings) > 0:\r\n",
    "        positive_sample_union = positive_sample[all_union_idxs]\r\n",
    "        positive_embedding = torch.index_select(model.entity_embedding, dim=0, index=positive_sample_union).unsqueeze(1).unsqueeze(1)\r\n",
    "        positive_union_logit = model.cal_logit_vec(positive_embedding, all_union_center_embeddings)\r\n",
    "        positive_union_logit = torch.max(positive_union_logit, dim=1)[0]\r\n",
    "    else:\r\n",
    "        positive_union_logit = torch.Tensor([]).to(model.entity_embedding.device)\r\n",
    "    positive_logit = torch.cat([positive_logit, positive_union_logit], dim=0)\r\n",
    "else:\r\n",
    "    positive_logit = None\r\n",
    "\r\n",
    "# in test, negative samples are basically all possible list of entities\r\n",
    "if type(negative_sample) != type(None):\r\n",
    "    if len(all_center_embeddings) > 0:\r\n",
    "        negative_sample_regular = negative_sample[all_idxs]\r\n",
    "        batch_size, negative_size = negative_sample_regular.shape\r\n",
    "        negative_embedding = torch.index_select(model.entity_embedding, dim=0, index=negative_sample_regular.view(-1)).view(batch_size, negative_size, -1)\r\n",
    "        negative_logit = model.cal_logit_vec(negative_embedding, all_center_embeddings)\r\n",
    "    else:\r\n",
    "        negative_logit = torch.Tensor([]).to(model.entity_embedding.device)\r\n",
    "\r\n",
    "    if len(all_union_center_embeddings) > 0:\r\n",
    "        negative_sample_union = negative_sample[all_union_idxs]\r\n",
    "        batch_size, negative_size = negative_sample_union.shape\r\n",
    "        negative_embedding = torch.index_select(model.entity_embedding, dim=0, index=negative_sample_union.view(-1)).view(batch_size, 1, negative_size, -1)\r\n",
    "        negative_union_logit = model.cal_logit_vec(negative_embedding, all_union_center_embeddings)\r\n",
    "        negative_union_logit = torch.max(negative_union_logit, dim=1)[0]\r\n",
    "    else:\r\n",
    "        negative_union_logit = torch.Tensor([]).to(model.entity_embedding.device)\r\n",
    "    negative_logit = torch.cat([negative_logit, negative_union_logit], dim=0)\r\n",
    "else:\r\n",
    "    negative_logit = None\r\n",
    "\r\n",
    "positive_logit, negative_logit, subsampling_weight, all_idxs+all_union_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a347bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0005,  0.0071, -0.0040,  ..., -0.0015,  0.0015, -0.0132],\n",
       "         [-0.0148,  0.0471,  0.0523,  ...,  0.0172, -0.0143,  0.0200],\n",
       "         [ 0.0341, -0.0213, -0.0102,  ..., -0.0171,  0.0311, -0.0060]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.embed_query_vec in models.py\"\"\"\r\n",
    "# get the query embedding\r\n",
    "# input is (batch_size, query_type_length=2), query_structure=(e, r)\r\n",
    "# model.embed_query_vec(batch_queries_dict[query_structure], query_structure, 0)\r\n",
    "\r\n",
    "queries, query_structure, idx = batch_queries_dict[query_structure], query_structure, 0\r\n",
    "\r\n",
    "all_relation_flag = True\r\n",
    "for ele in query_structure[-1]:\r\n",
    "    if ele not in ['r', 'n']:\r\n",
    "        all_relation_flag = False\r\n",
    "        break\r\n",
    "if all_relation_flag:\r\n",
    "    # if anchor\r\n",
    "    if query_structure[0] == \"e\":\r\n",
    "        # get entity embedding\r\n",
    "        embedding = torch.index_select(model.entity_embedding, dim=0, index=queries[:, idx])\r\n",
    "        idx += 1\r\n",
    "    else:\r\n",
    "        # recursion\r\n",
    "        embedding, idx = model.embed_query_vec(queries, query_structure[0], idx)\r\n",
    "    for i in range(len(query_structure[-1])):\r\n",
    "        if query_structure[-1][i] == \"n\":\r\n",
    "            assert False, \"vec cannot handle queries with negation\"\r\n",
    "        else:\r\n",
    "            # get relation embedding\r\n",
    "            r_embedding = torch.index_select(model.relation_embedding, dim=0, index=queries[:, idx])\r\n",
    "            # add entity and relation embedding\r\n",
    "            embedding += r_embedding\r\n",
    "        idx += 1\r\n",
    "else:\r\n",
    "    embedding_list = []\r\n",
    "    for i in range(len(query_structure)):\r\n",
    "        embedding, idx = model.embed_query_vec(queries, query_structure[i], idx)\r\n",
    "        embedding_list.append(embedding)\r\n",
    "    embedding = model.center_net(torch.stack(embedding_list))\r\n",
    "    \r\n",
    "embedding, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ce700c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14505, 500]) torch.Size([3, 1, 500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 14505, 500]), torch.Size([3, 14505]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" model.cal_logit_vec \"\"\"\r\n",
    "# score / calculate distance between candidate embedding and query embedding\r\n",
    "# negative \r\n",
    "print(negative_embedding.shape, all_center_embeddings.shape)\r\n",
    "# model.cal_logit_vec(negative_embedding, all_center_embeddings)\r\n",
    "\r\n",
    "#     def cal_logit_vec(self, entity_embedding, query_embedding):\r\n",
    "#         distance = entity_embedding - query_embedding\r\n",
    "#         logit = self.gamma - torch.norm(distance, p=1, dim=-1)\r\n",
    "#         return logit\r\n",
    "\r\n",
    "entity_embedding, query_embedding = negative_embedding, all_center_embeddings\r\n",
    "distance = entity_embedding - query_embedding\r\n",
    "logit = model.gamma - torch.norm(distance, p=1, dim=-1)\r\n",
    "\r\n",
    "distance.shape, logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4248358f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1718, 4586, 8803], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.argmax(axis=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "343fdc80cc68c688baad9e163e1f88e33cc019c87aed1c9258d7ce7a9f5d5e41"
  },
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "my-rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
