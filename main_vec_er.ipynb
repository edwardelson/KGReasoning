{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f02193",
   "metadata": {},
   "source": [
    "based on main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ff12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\r\n",
    "from __future__ import division\r\n",
    "from __future__ import print_function\r\n",
    "\r\n",
    "import argparse\r\n",
    "import json\r\n",
    "import logging\r\n",
    "import os\r\n",
    "import random\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from models import KGReasoning\r\n",
    "from dataloader import TestDataset, TrainDataset, SingledirectionalOneShotIterator\r\n",
    "# from tensorboardX import SummaryWriter\r\n",
    "import time\r\n",
    "import pickle\r\n",
    "from collections import defaultdict\r\n",
    "from tqdm import tqdm\r\n",
    "from util import flatten_query, list2tuple, parse_time, set_global_seed, eval_tuple\r\n",
    "\r\n",
    "import collections\r\n",
    "import random\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "\r\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e633a91",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c9759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', 'up-DNF', '2u-DM', 'up-DM']\n"
     ]
    }
   ],
   "source": [
    "query_name_dict = {('e',('r',)): '1p', \r\n",
    "                    ('e', ('r', 'r')): '2p',\r\n",
    "                    ('e', ('r', 'r', 'r')): '3p',\r\n",
    "                    (('e', ('r',)), ('e', ('r',))): '2i',\r\n",
    "                    (('e', ('r',)), ('e', ('r',)), ('e', ('r',))): '3i',\r\n",
    "                    ((('e', ('r',)), ('e', ('r',))), ('r',)): 'ip',\r\n",
    "                    (('e', ('r', 'r')), ('e', ('r',))): 'pi',\r\n",
    "                    (('e', ('r',)), ('e', ('r', 'n'))): '2in',\r\n",
    "                    (('e', ('r',)), ('e', ('r',)), ('e', ('r', 'n'))): '3in',\r\n",
    "                    ((('e', ('r',)), ('e', ('r', 'n'))), ('r',)): 'inp',\r\n",
    "                    (('e', ('r', 'r')), ('e', ('r', 'n'))): 'pin',\r\n",
    "                    (('e', ('r', 'r', 'n')), ('e', ('r',))): 'pni',\r\n",
    "                    (('e', ('r',)), ('e', ('r',)), ('u',)): '2u-DNF',\r\n",
    "                    ((('e', ('r',)), ('e', ('r',)), ('u',)), ('r',)): 'up-DNF',\r\n",
    "                    ((('e', ('r', 'n')), ('e', ('r', 'n'))), ('n',)): '2u-DM',\r\n",
    "                    ((('e', ('r', 'n')), ('e', ('r', 'n'))), ('n', 'r')): 'up-DM'\r\n",
    "                }\r\n",
    "name_query_dict = {value: key for key, value in query_name_dict.items()}\r\n",
    "all_tasks = list(name_query_dict.keys()) # ['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', '2u-DM', 'up-DNF', 'up-DM']\r\n",
    "\r\n",
    "print(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee2dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyArgs:\r\n",
    "    def __init__(self):\r\n",
    "        None\r\n",
    "        \r\n",
    "args = DummyArgs()\r\n",
    "args.cuda = True\r\n",
    "args.geo = \"vec\" # choices=['vec', 'box', 'beta']\r\n",
    "args.gamma = 12.0\r\n",
    "args.box_mode = \"(none,0.02)\" # Query2box\r\n",
    "args.beta_mode = \"(1600,2)\" # BetaE relational projection\r\n",
    "args.data_path = \"data/FB15k-237-betae\"\r\n",
    "args.batch_size = 64 #1024\r\n",
    "args.cpu_num = 1# 10\r\n",
    "args.negative_sample_size = 128\r\n",
    "args.hidden_dim = 500\r\n",
    "args.test_batch_size = 3 #2\r\n",
    "args.print_on_screen = True\r\n",
    "args.test_log_steps = 1000\r\n",
    "args.learning_rate = 0.0001\r\n",
    "args.max_steps = 100000\r\n",
    "args.evaluate_union = \"DNF\" # choices=['DNF', 'DM'] evaluate union querioes, disjunctive normal form (DNF) or de Morgan's laws (DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f196e063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', 'up-DNF', '2u-DM', 'up-DM']\n"
     ]
    }
   ],
   "source": [
    "# different query types\r\n",
    "tasks = all_tasks[0:1]\r\n",
    "\r\n",
    "if args.geo in ['box']:\r\n",
    "    tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.box_mode)\r\n",
    "elif args.geo in ['vec']:\r\n",
    "    tmp_str = \"g-{}\".format(args.gamma)\r\n",
    "elif args.geo == 'beta':\r\n",
    "    tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.beta_mode)\r\n",
    "\r\n",
    "print(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a815917",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/stats.txt' % args.data_path) as f:\r\n",
    "    entrel = f.readlines()\r\n",
    "    nentity = int(entrel[0].split(' ')[-1])\r\n",
    "    nrelation = int(entrel[1].split(' ')[-1])\r\n",
    "\r\n",
    "args.nentity = nentity\r\n",
    "args.nrelation = nrelation\r\n",
    "\r\n",
    "logging.info('-------------------------------'*3)\r\n",
    "logging.info('Geo: %s' % args.geo)\r\n",
    "logging.info('Data Path: %s' % args.data_path)\r\n",
    "logging.info('#entity: %d' % nentity)\r\n",
    "logging.info('#relation: %d' % nrelation)\r\n",
    "# logging.info('#max steps: %d' % args.max_steps)\r\n",
    "# logging.info('Evaluate unoins using: %s' % args.evaluate_union)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a44ce1",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "693ef208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, tasks):\r\n",
    "    '''\r\n",
    "    Load queries and remove queries not in tasks\r\n",
    "    '''\r\n",
    "    logging.info(\"loading data\")\r\n",
    "    train_queries = pickle.load(open(os.path.join(args.data_path, \"train-queries.pkl\"), 'rb'))\r\n",
    "    train_answers = pickle.load(open(os.path.join(args.data_path, \"train-answers.pkl\"), 'rb'))\r\n",
    "    valid_queries = pickle.load(open(os.path.join(args.data_path, \"valid-queries.pkl\"), 'rb'))\r\n",
    "    valid_hard_answers = pickle.load(open(os.path.join(args.data_path, \"valid-hard-answers.pkl\"), 'rb'))\r\n",
    "    valid_easy_answers = pickle.load(open(os.path.join(args.data_path, \"valid-easy-answers.pkl\"), 'rb'))\r\n",
    "    test_queries = pickle.load(open(os.path.join(args.data_path, \"test-queries.pkl\"), 'rb'))\r\n",
    "    test_hard_answers = pickle.load(open(os.path.join(args.data_path, \"test-hard-answers.pkl\"), 'rb'))\r\n",
    "    test_easy_answers = pickle.load(open(os.path.join(args.data_path, \"test-easy-answers.pkl\"), 'rb'))\r\n",
    "    \r\n",
    "    # remove tasks not in args.tasks\r\n",
    "    for name in all_tasks:\r\n",
    "        if 'u' in name:\r\n",
    "            name, evaluate_union = name.split('-')\r\n",
    "        else:\r\n",
    "            evaluate_union = args.evaluate_union\r\n",
    "        if name not in tasks or evaluate_union != args.evaluate_union:\r\n",
    "            query_structure = name_query_dict[name if 'u' not in name else '-'.join([name, evaluate_union])]\r\n",
    "            if query_structure in train_queries:\r\n",
    "                del train_queries[query_structure]\r\n",
    "            if query_structure in valid_queries:\r\n",
    "                del valid_queries[query_structure]\r\n",
    "            if query_structure in test_queries:\r\n",
    "                del test_queries[query_structure]\r\n",
    "\r\n",
    "    return train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2ff51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\r\n",
    "LOAD DATA\r\n",
    "based on tasks defined in args\r\n",
    "\"\"\"\r\n",
    "train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers = load_data(args, tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a310f",
   "metadata": {},
   "source": [
    "1. train_queries: for (e, r) given entityID and relationID, find all other entities that are connected to it.\n",
    "2. train_answers: given (e, r) this is the groundtruth answers of all entityIDs that are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b59dd1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(collections.defaultdict, 1),\n",
       " (collections.defaultdict, 1496890),\n",
       " (collections.defaultdict, 1),\n",
       " (collections.defaultdict, 95094),\n",
       " (collections.defaultdict, 95094),\n",
       " (collections.defaultdict, 1),\n",
       " (collections.defaultdict, 97804),\n",
       " (collections.defaultdict, 97804)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(type(x), len(x)) for x in [train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "635ee31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "(6288, (8,))\n",
      "False\n",
      "valid_hard_answers set()\n",
      "valid_easy_answers set()\n",
      "**********\n",
      "(6288, (8,))\n",
      "True\n",
      "train_answers {9, 11, 117, 399}\n",
      "**********\n",
      "(6288, (8,))\n",
      "False\n",
      "test_hard_answers set()\n",
      "test_easy_answers set()\n"
     ]
    }
   ],
   "source": [
    "ex_val_q = (6288, (8,))\r\n",
    "print(\"*\" * 10)\r\n",
    "print(ex_val_q)\r\n",
    "print(ex_val_q in valid_queries[('e', ('r', ))])\r\n",
    "print(\"valid_hard_answers\", valid_hard_answers[ex_val_q])\r\n",
    "print(\"valid_easy_answers\", valid_easy_answers[ex_val_q])\r\n",
    "\r\n",
    "# get example query\r\n",
    "ex_q = ex_val_q #(6288, (8,))\r\n",
    "print(\"*\" * 10)\r\n",
    "print(ex_q)\r\n",
    "print(ex_q in train_queries[('e', ('r', ))])\r\n",
    "print(\"train_answers\", train_answers[ex_q])\r\n",
    "\r\n",
    "ex_test_q = ex_val_q\r\n",
    "print(\"*\" * 10)\r\n",
    "print(ex_test_q)\r\n",
    "print(ex_test_q in test_queries[('e', ('r', ))])\r\n",
    "print(\"test_hard_answers\", test_hard_answers[ex_val_q])\r\n",
    "print(\"test_easy_answers\", test_easy_answers[ex_val_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "888dc79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train_queries 149689\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "TRAINING Data Preparation\r\n",
    "\"\"\"\r\n",
    "train_path_queries = defaultdict(set)\r\n",
    "for query_structure in train_queries:\r\n",
    "#     train_path_queries[query_structure] = set(random.sample(train_queries[query_structure], 1))\r\n",
    "    train_path_queries[query_structure] = train_queries[query_structure]\r\n",
    "            \r\n",
    "train_path_queries = flatten_query(train_path_queries)\r\n",
    "print(\"number of train_queries\", len(train_path_queries))\r\n",
    "train_path_iterator = SingledirectionalOneShotIterator(DataLoader(\r\n",
    "                            # {(e,r): (entityID, relID)} and {(e, r): [entityID]}\r\n",
    "                            TrainDataset(train_path_queries, nentity, nrelation, args.negative_sample_size, train_answers),\r\n",
    "                            batch_size=args.batch_size,\r\n",
    "                            shuffle=True,\r\n",
    "                            num_workers=args.cpu_num,\r\n",
    "                            collate_fn=TrainDataset.collate_fn\r\n",
    "                        ))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c81c600",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error ('e', ('r',))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "VALIDATION Data Preparation\r\n",
    "\"\"\"\r\n",
    "# for each type of structure / key\r\n",
    "for query_structure in valid_queries:\r\n",
    "    try:\r\n",
    "        print(query_name_dict[query_structure[1]]+\": \"+str(len(valid_queries[query_structure])))\r\n",
    "    except:\r\n",
    "        print(\"error\", query_structure)\r\n",
    "\r\n",
    "valid_dataloader = DataLoader(\r\n",
    "    TestDataset(\r\n",
    "        flatten_query(valid_queries), \r\n",
    "        args.nentity, \r\n",
    "        args.nrelation, \r\n",
    "    ), \r\n",
    "    batch_size=args.test_batch_size,\r\n",
    "    num_workers=args.cpu_num, \r\n",
    "    collate_fn=TestDataset.collate_fn\r\n",
    ")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2655db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter gamma: torch.Size([1]), require_grad = False\n",
      "Parameter embedding_range: torch.Size([1]), require_grad = False\n",
      "Parameter entity_embedding: torch.Size([14505, 500]), require_grad = True\n",
      "Parameter relation_embedding: torch.Size([474, 500]), require_grad = True\n",
      "Parameter center_net.layer1.weight: torch.Size([500, 500]), require_grad = True\n",
      "Parameter center_net.layer1.bias: torch.Size([500]), require_grad = True\n",
      "Parameter center_net.layer2.weight: torch.Size([500, 500]), require_grad = True\n",
      "Parameter center_net.layer2.bias: torch.Size([500]), require_grad = True\n",
      "Parameter Number: 7990500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "MODEL DEFINITION\r\n",
    "\"\"\"\r\n",
    "model = KGReasoning(\r\n",
    "    nentity=nentity,\r\n",
    "    nrelation=nrelation,\r\n",
    "    hidden_dim=args.hidden_dim,\r\n",
    "    gamma=args.gamma,\r\n",
    "    geo=args.geo,\r\n",
    "    use_cuda = args.cuda,\r\n",
    "    box_mode=eval_tuple(args.box_mode),\r\n",
    "    beta_mode = eval_tuple(args.beta_mode),\r\n",
    "    test_batch_size=args.test_batch_size,\r\n",
    "    query_name_dict = query_name_dict\r\n",
    ")\r\n",
    "\r\n",
    "logging.info('Model Parameter Configuration:')\r\n",
    "num_params = 0\r\n",
    "for name, param in model.named_parameters():\r\n",
    "    print('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\r\n",
    "    if param.requires_grad:\r\n",
    "        num_params += np.prod(param.size())\r\n",
    "print('Parameter Number: %d' % num_params)\r\n",
    "\r\n",
    "model = model.cuda()\r\n",
    "\r\n",
    "current_learning_rate = args.learning_rate\r\n",
    "optimizer = torch.optim.Adam(\r\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \r\n",
    "    lr=current_learning_rate\r\n",
    ")\r\n",
    "warm_up_steps = args.max_steps // 2\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56007bed",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02cae23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: eekosasih (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">2021-09-08T13:26:27.544166</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/eekosasih/KGReasoning\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/eekosasih/KGReasoning/runs/2okuj3ns\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning/runs/2okuj3ns</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210908_142629-2okuj3ns</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2okuj3ns)</h1><iframe src=\"https://wandb.ai/eekosasih/KGReasoning/runs/2okuj3ns\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1fdd800ffc8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\r\n",
    "import datetime\r\n",
    "\r\n",
    "timenow = datetime.datetime.utcnow().isoformat()\r\n",
    "\r\n",
    "wandb.init(project=\"KGReasoning\", name=timenow, group=\"vec\")\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa053e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca2954ed2ca4f4eaa10c102b2a0dbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4a4dc5ad3e96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# train model for a step over all batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_path_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtraining_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\KGReasoning\\models.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, optimizer, train_iterator, args, step)\u001b[0m\n\u001b[0;32m    867\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpositive_sample_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnegative_sample_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         log = {\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "TRAIN LOOP\r\n",
    "\"\"\"\r\n",
    "init_step = 0\r\n",
    "\r\n",
    "training_logs = []\r\n",
    "# loop over all batches\r\n",
    "# initialising train_path_iterator is expensive. So at certain loop it might take some time to load\r\n",
    "for step in tqdm(range(init_step, args.max_steps)):\r\n",
    "    # train model for a step over all batches\r\n",
    "    log = model.train_step(model, optimizer, train_path_iterator, args, step)\r\n",
    "    training_logs.append(log)\r\n",
    "\r\n",
    "    log[\"epoch\"] = step\r\n",
    "    wandb.log(log, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bc57c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\r\n",
    "torch.save(\r\n",
    "    {\r\n",
    "        'model_state_dict': model.state_dict(),\r\n",
    "        'optimizer_state_dict': optimizer.state_dict()\r\n",
    "    },\r\n",
    "    os.path.join(\"checkpoint\", \"model.pt\")\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03889aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a213b71",
   "metadata": {},
   "source": [
    "## DEBUG TRAINING STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "306da2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive_sample_loss': 0.39944127202033997,\n",
       " 'negative_sample_loss': 1.0847606658935547,\n",
       " 'loss': 0.7421009540557861}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\r\n",
    "\r\n",
    "# model.train_step(model, optimizer, train_other_iterator, args, step)\r\n",
    "\r\n",
    "model, optimizer, train_iterator, args, step = model, optimizer, train_path_iterator, args, step\r\n",
    "\r\n",
    "model.train()\r\n",
    "optimizer.zero_grad()\r\n",
    "\r\n",
    "# there's an overhead operation to shufflte the train_iterator that makes this expensive to run\r\n",
    "positive_sample, negative_sample, subsampling_weight, batch_queries, query_structures = next(train_iterator)\r\n",
    "\r\n",
    "# group queries into batch\r\n",
    "batch_queries_dict = collections.defaultdict(list)\r\n",
    "batch_idxs_dict = collections.defaultdict(list)\r\n",
    "for i, query in enumerate(batch_queries): # group queries with same structure\r\n",
    "    batch_queries_dict[query_structures[i]].append(query)\r\n",
    "    batch_idxs_dict[query_structures[i]].append(i)\r\n",
    "\r\n",
    "for query_structure in batch_queries_dict:\r\n",
    "    if args.cuda:\r\n",
    "        batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure]).cuda()\r\n",
    "    else:\r\n",
    "        batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure])\r\n",
    "\r\n",
    "if args.cuda:\r\n",
    "    positive_sample = positive_sample.cuda()\r\n",
    "    negative_sample = negative_sample.cuda()\r\n",
    "    subsampling_weight = subsampling_weight.cuda()\r\n",
    "\r\n",
    "# score positive and negative samples\r\n",
    "positive_logit, negative_logit, subsampling_weight, _ = model(positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict)\r\n",
    "\r\n",
    "# calculate loss\r\n",
    "# positive samples should have label of 1 while negative samples label of 0\r\n",
    "negative_score = F.logsigmoid(-negative_logit).mean(dim=1)\r\n",
    "positive_score = F.logsigmoid(positive_logit).mean(dim=1)\r\n",
    "# aggregate loss with subsampling_weight\r\n",
    "positive_sample_loss = - (subsampling_weight * positive_score).sum()\r\n",
    "negative_sample_loss = - (subsampling_weight * negative_score).sum()\r\n",
    "positive_sample_loss /= subsampling_weight.sum()\r\n",
    "negative_sample_loss /= subsampling_weight.sum()\r\n",
    "loss = (positive_sample_loss + negative_sample_loss)/2\r\n",
    "\r\n",
    "loss.backward()\r\n",
    "optimizer.step()\r\n",
    "\r\n",
    "log = {\r\n",
    "    'positive_sample_loss': positive_sample_loss.item(),\r\n",
    "    'negative_sample_loss': negative_sample_loss.item(),\r\n",
    "    'loss': loss.item(),\r\n",
    "}\r\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2709292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== input ===\n",
      "torch.Size([64]) torch.Size([64, 128]) 64 64 torch.Size([64])\n",
      "=== output ===\n",
      "torch.Size([64, 1]) torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== input ===\")\n",
    "print(positive_sample.shape, negative_sample.shape, len(batch_queries), len(query_structures), subsampling_weight.shape)\n",
    "print(\"=== output ===\")\n",
    "print(positive_logit.shape, negative_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4ae2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8750ab6c",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea498d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model\n",
    "checkpoint = torch.load(os.path.join(\"checkpoint\", \"model.pt\"))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "347aefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tp_answers, fn_answers, args, dataloader, query_name_dict, mode, step):\n",
    "    '''\n",
    "    Evaluate queries in dataloader\n",
    "    '''\n",
    "    average_metrics = defaultdict(float)\n",
    "    all_metrics = defaultdict(float)\n",
    "\n",
    "    metrics = model.test_step(model, tp_answers, fn_answers, args, dataloader, query_name_dict)\n",
    "    num_query_structures = 0\n",
    "    num_queries = 0\n",
    "    for query_structure in metrics:\n",
    "        log_metrics(mode+\" \"+query_name_dict[query_structure], step, metrics[query_structure])\n",
    "        for metric in metrics[query_structure]:\n",
    "            all_metrics[\"_\".join([query_name_dict[query_structure], metric])] = metrics[query_structure][metric]\n",
    "            if metric != 'num_queries':\n",
    "                average_metrics[metric] += metrics[query_structure][metric]\n",
    "        num_queries += metrics[query_structure]['num_queries']\n",
    "        num_query_structures += 1\n",
    "\n",
    "    for metric in average_metrics:\n",
    "        average_metrics[metric] /= num_query_structures\n",
    "        all_metrics[\"_\".join([\"average\", metric])] = average_metrics[metric]\n",
    "    log_metrics('%s average'%mode, step, average_metrics)\n",
    "\n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "635751cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(mode, step, metrics):\n",
    "    '''\n",
    "    Print the evaluation logs\n",
    "    '''\n",
    "    for metric in metrics:\n",
    "        logging.info('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "896ab4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [01:05<00:00, 103.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'1p_MRR': 0.00955718961143577,\n",
       "             '1p_HITS1': 0.008909909141400957,\n",
       "             '1p_HITS3': 0.009103772420567032,\n",
       "             '1p_HITS10': 0.009542757725153821,\n",
       "             '1p_num_queries': 20094,\n",
       "             'average_MRR': 0.00955718961143577,\n",
       "             'average_HITS1': 0.008909909141400957,\n",
       "             'average_HITS3': 0.009103772420567032,\n",
       "             'average_HITS10': 0.009542757725153821})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea083719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [01:06<00:00, 100.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'1p_MRR': 0.366261465070138,\n",
       "             '1p_HITS1': 0.2592751371505161,\n",
       "             '1p_HITS3': 0.41276127284368985,\n",
       "             '1p_HITS10': 0.5799358917393381,\n",
       "             '1p_num_queries': 20094,\n",
       "             'average_MRR': 0.366261465070138,\n",
       "             'average_HITS1': 0.2592751371505161,\n",
       "             'average_HITS3': 0.41276127284368985,\n",
       "             'average_HITS10': 0.5799358917393381})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"`\n",
    "EVAL\n",
    "\"\"\"\n",
    "step = -1\n",
    "valid_all_metrics = evaluate(model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict, 'Valid', step)\n",
    "valid_all_metrics\n",
    "\n",
    "# after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5420c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd9233dc",
   "metadata": {},
   "source": [
    "## DEBUG TEST STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f95e7db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [01:07<00:00, 99.49it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function models.KGReasoning.test_step.<locals>.<lambda>()>,\n",
       "            {('e', ('r',)): defaultdict(int,\n",
       "                         {'MRR': 0.009518244747182089,\n",
       "                          'HITS1': 0.008909909141400957,\n",
       "                          'HITS3': 0.009125562239005588,\n",
       "                          'HITS10': 0.009491169651853655,\n",
       "                          'num_queries': 20094})})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def evaluate in main.py\"\"\"\n",
    "\n",
    "metrics = model.test_step(model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2fb2264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38656f364568485e829223a8155cf66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "         [    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "         [    0,     1,     2,  ..., 14502, 14503, 14504]]),\n",
       " [[11217, 143], [5673, 34], [4190, 62]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for negative_sample, queries, queries_unflatten, query_structures in tqdm(valid_dataloader, disable=not args.print_on_screen):\n",
    "    break\n",
    "    \n",
    "negative_sample, queries, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d4d97",
   "metadata": {},
   "source": [
    "1. why is there no positive_sample in test_dataloader\n",
    "2. how to embed query vector and perform projection & intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7295a374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "        [    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "        [    0,     1,     2,  ..., 14502, 14503, 14504]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15d36cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd9a696432d44d59c60123ec978ca1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {('e', ('r',)): defaultdict(int,\n",
       "                         {'MRR': 0.009518244747182089,\n",
       "                          'HITS1': 0.008909909141400957,\n",
       "                          'HITS3': 0.009125562239005588,\n",
       "                          'HITS10': 0.009491169651853655,\n",
       "                          'num_queries': 20094})})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.test_step in models.py\"\"\"\n",
    "model, easy_answers, hard_answers, args, test_dataloader, query_name_dict = model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict\n",
    "\n",
    "model.eval()\n",
    "\n",
    "step = 0\n",
    "total_steps = len(test_dataloader)\n",
    "logs = collections.defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # process per batch\n",
    "    # loop through every validation dataset\n",
    "    # negative sample is basically all entities in nentity\n",
    "    for negative_sample, queries, queries_unflatten, query_structures in tqdm(test_dataloader, disable=not args.print_on_screen):\n",
    "        batch_queries_dict = collections.defaultdict(list)\n",
    "        batch_idxs_dict = collections.defaultdict(list)\n",
    "        \n",
    "        # for each test query\n",
    "        for i, query in enumerate(queries):\n",
    "            batch_queries_dict[query_structures[i]].append(query)\n",
    "            batch_idxs_dict[query_structures[i]].append(i)\n",
    "            \n",
    "        # convert each positive query to a LongTensor\n",
    "        for query_structure in batch_queries_dict:\n",
    "            if args.cuda:\n",
    "                batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure]).cuda()\n",
    "            else:\n",
    "                batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure])\n",
    "            \n",
    "        # convert each negative query to a LongTensor\n",
    "        if args.cuda:\n",
    "            negative_sample = negative_sample.cuda()\n",
    "            \n",
    "        # get prediction output\n",
    "        # (2, number_of_edges) same dimension with negative_sample\n",
    "        # this is basically embedding lookup\n",
    "        \"\"\"model.forward\"\"\"\n",
    "        # negative_sample here is just a list of all entities, some of them are actually positive\n",
    "        _, negative_logit, _, idxs = model(None, negative_sample, None, batch_queries_dict, batch_idxs_dict)\n",
    "          \n",
    "        # ... scoring ...\n",
    "        queries_unflatten = [queries_unflatten[i] for i in idxs]\n",
    "        query_structures = [query_structures[i] for i in idxs]\n",
    "        # sort from maximum value based on logit values\n",
    "        argsort = torch.argsort(negative_logit, dim=1, descending=True)\n",
    "        ranking = argsort.clone().to(torch.float)\n",
    "        if len(argsort) == args.test_batch_size: # if it is the same shape with test_batch_size, we can reuse batch_entity_range without creating a new one\n",
    "            ranking = ranking.scatter_(1, argsort, model.batch_entity_range) # achieve the ranking of all entities\n",
    "        else: # otherwise, create a new torch Tensor for batch_entity_range\n",
    "            if args.cuda:\n",
    "                ranking = ranking.scatter_(1, argsort, torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0], 1).cuda()) # achieve the ranking of all entities\n",
    "            else:\n",
    "                ranking = ranking.scatter_(1, argsort, torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0], 1)) # achieve the ranking of all entities\n",
    "        \n",
    "        # loop through ranking\n",
    "        # score every query in the dataloader batch\n",
    "        for idx, (i, query, query_structure) in enumerate(zip(argsort[:, 0], queries_unflatten, query_structures)):\n",
    "            # get groundtruth labels\n",
    "            hard_answer = hard_answers[query]\n",
    "            easy_answer = easy_answers[query]\n",
    "            num_hard = len(hard_answer)\n",
    "            num_easy = len(easy_answer)\n",
    "            assert len(hard_answer.intersection(easy_answer)) == 0\n",
    "\n",
    "            # compare ranking of groundtruth (easy_answer, hard_answer) and predicted results\n",
    "            cur_ranking = ranking[idx, list(easy_answer) + list(hard_answer)]\n",
    "            cur_ranking, indices = torch.sort(cur_ranking)\n",
    "            masks = indices >= num_easy\n",
    "            if args.cuda:\n",
    "                answer_list = torch.arange(num_hard + num_easy).to(torch.float).cuda()\n",
    "            else:\n",
    "                answer_list = torch.arange(num_hard + num_easy).to(torch.float)\n",
    "            cur_ranking = cur_ranking - answer_list + 1 # filtered setting\n",
    "            cur_ranking = cur_ranking[masks] # only take indices that belong to the hard answers\n",
    "\n",
    "            mrr = torch.mean(1./cur_ranking).item()\n",
    "            h1 = torch.mean((cur_ranking <= 1).to(torch.float)).item()\n",
    "            h3 = torch.mean((cur_ranking <= 3).to(torch.float)).item()\n",
    "            h10 = torch.mean((cur_ranking <= 10).to(torch.float)).item()\n",
    "\n",
    "            logs[query_structure].append({\n",
    "                'MRR': mrr,\n",
    "                'HITS1': h1,\n",
    "                'HITS3': h3,\n",
    "                'HITS10': h10,\n",
    "                'num_hard_answer': num_hard,\n",
    "            })\n",
    "\n",
    "metrics = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "for query_structure in logs:\n",
    "    for metric in logs[query_structure][0].keys():\n",
    "        if metric in ['num_hard_answer']:\n",
    "            continue\n",
    "        metrics[query_structure][metric] = sum([log[metric] for log in logs[query_structure]])/len(logs[query_structure])\n",
    "    metrics[query_structure]['num_queries'] = len(logs[query_structure])\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9174a544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[0.8224, 0.7421, 0.8782,  ..., 0.5703, 0.7527, 0.5376],\n",
       "         [0.5680, 0.4127, 0.2810,  ..., 0.4131, 0.6748, 0.3881],\n",
       "         [0.6553, 1.1643, 1.2482,  ..., 0.8182, 1.0112, 1.0331]],\n",
       "        device='cuda:0', grad_fn=<CatBackward>),\n",
       " None,\n",
       " [0, 1, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.forward\"\"\"\n",
    "# model.forward(self, positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict)\n",
    "\n",
    "# model.geo == \"vec\"\n",
    "\n",
    "# done per batch\n",
    "\"\"\"model.forward_vec\"\"\"\n",
    "# model.forward_vec(self, positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict)\n",
    "# negative_sample is a list of entities to be scored as potential answers to the queries\n",
    "# batch_queries_dict stores list of queries (in this case (e, r) to be scored)\n",
    "positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict = None, negative_sample, None, batch_queries_dict, batch_idxs_dict\n",
    "\n",
    "all_center_embeddings, all_idxs = [], []\n",
    "all_union_center_embeddings, all_union_idxs = [], []\n",
    "for query_structure in batch_queries_dict:\n",
    "    if 'u' in model.query_name_dict[query_structure]:\n",
    "        # embedding \n",
    "        center_embedding, _ = model.embed_query_vec(model.transform_union_query(batch_queries_dict[query_structure], \n",
    "                                                            query_structure), \n",
    "                                                        model.transform_union_structure(query_structure), 0)\n",
    "        all_union_center_embeddings.append(center_embedding)\n",
    "        all_union_idxs.extend(batch_idxs_dict[query_structure])\n",
    "    else:\n",
    "        # get vector embedding for each query (anchor + projection + intersection)\n",
    "        # this will be compared with the groundtruth answer entity embedding\n",
    "        center_embedding, _ = model.embed_query_vec(batch_queries_dict[query_structure], query_structure, 0)\n",
    "        all_center_embeddings.append(center_embedding)\n",
    "        all_idxs.extend(batch_idxs_dict[query_structure])\n",
    "\n",
    "if len(all_center_embeddings) > 0:\n",
    "    all_center_embeddings = torch.cat(all_center_embeddings, dim=0).unsqueeze(1)\n",
    "if len(all_union_center_embeddings) > 0:\n",
    "    all_union_center_embeddings = torch.cat(all_union_center_embeddings, dim=0).unsqueeze(1)\n",
    "    all_union_center_embeddings = all_union_center_embeddings.view(all_union_center_embeddings.shape[0]//2, 2, 1, -1)\n",
    "\n",
    "if type(subsampling_weight) != type(None):\n",
    "    subsampling_weight = subsampling_weight[all_idxs+all_union_idxs]\n",
    "\n",
    "# in test, positive samples are not given\n",
    "if type(positive_sample) != type(None):\n",
    "    if len(all_center_embeddings) > 0:\n",
    "        positive_sample_regular = positive_sample[all_idxs]\n",
    "        positive_embedding = torch.index_select(model.entity_embedding, dim=0, index=positive_sample_regular).unsqueeze(1)\n",
    "        positive_logit = model.cal_logit_vec(positive_embedding, all_center_embeddings)\n",
    "    else:\n",
    "        positive_logit = torch.Tensor([]).to(model.entity_embedding.device)\n",
    "\n",
    "    if len(all_union_center_embeddings) > 0:\n",
    "        positive_sample_union = positive_sample[all_union_idxs]\n",
    "        positive_embedding = torch.index_select(model.entity_embedding, dim=0, index=positive_sample_union).unsqueeze(1).unsqueeze(1)\n",
    "        positive_union_logit = model.cal_logit_vec(positive_embedding, all_union_center_embeddings)\n",
    "        positive_union_logit = torch.max(positive_union_logit, dim=1)[0]\n",
    "    else:\n",
    "        positive_union_logit = torch.Tensor([]).to(model.entity_embedding.device)\n",
    "    positive_logit = torch.cat([positive_logit, positive_union_logit], dim=0)\n",
    "else:\n",
    "    positive_logit = None\n",
    "\n",
    "# in test, negative samples are basically all possible list of entities\n",
    "if type(negative_sample) != type(None):\n",
    "    if len(all_center_embeddings) > 0:\n",
    "        negative_sample_regular = negative_sample[all_idxs]\n",
    "        batch_size, negative_size = negative_sample_regular.shape\n",
    "        negative_embedding = torch.index_select(model.entity_embedding, dim=0, index=negative_sample_regular.view(-1)).view(batch_size, negative_size, -1)\n",
    "        negative_logit = model.cal_logit_vec(negative_embedding, all_center_embeddings)\n",
    "    else:\n",
    "        negative_logit = torch.Tensor([]).to(model.entity_embedding.device)\n",
    "\n",
    "    if len(all_union_center_embeddings) > 0:\n",
    "        negative_sample_union = negative_sample[all_union_idxs]\n",
    "        batch_size, negative_size = negative_sample_union.shape\n",
    "        negative_embedding = torch.index_select(model.entity_embedding, dim=0, index=negative_sample_union.view(-1)).view(batch_size, 1, negative_size, -1)\n",
    "        negative_union_logit = model.cal_logit_vec(negative_embedding, all_union_center_embeddings)\n",
    "        negative_union_logit = torch.max(negative_union_logit, dim=1)[0]\n",
    "    else:\n",
    "        negative_union_logit = torch.Tensor([]).to(model.entity_embedding.device)\n",
    "    negative_logit = torch.cat([negative_logit, negative_union_logit], dim=0)\n",
    "else:\n",
    "    negative_logit = None\n",
    "\n",
    "positive_logit, negative_logit, subsampling_weight, all_idxs+all_union_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3fc9e0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0005,  0.0071, -0.0040,  ..., -0.0015,  0.0015, -0.0132],\n",
       "         [-0.0148,  0.0471,  0.0523,  ...,  0.0172, -0.0143,  0.0200],\n",
       "         [ 0.0341, -0.0213, -0.0102,  ..., -0.0171,  0.0311, -0.0060]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.embed_query_vec in models.py\"\"\"\n",
    "# get the query embedding\n",
    "# input is (batch_size, query_type_length=2), query_structure=(e, r)\n",
    "# model.embed_query_vec(batch_queries_dict[query_structure], query_structure, 0)\n",
    "\n",
    "queries, query_structure, idx = batch_queries_dict[query_structure], query_structure, 0\n",
    "\n",
    "all_relation_flag = True\n",
    "for ele in query_structure[-1]:\n",
    "    if ele not in ['r', 'n']:\n",
    "        all_relation_flag = False\n",
    "        break\n",
    "if all_relation_flag:\n",
    "    # if anchor\n",
    "    if query_structure[0] == \"e\":\n",
    "        # get entity embedding\n",
    "        embedding = torch.index_select(model.entity_embedding, dim=0, index=queries[:, idx])\n",
    "        idx += 1\n",
    "    else:\n",
    "        # recursion\n",
    "        embedding, idx = model.embed_query_vec(queries, query_structure[0], idx)\n",
    "    for i in range(len(query_structure[-1])):\n",
    "        if query_structure[-1][i] == \"n\":\n",
    "            assert False, \"vec cannot handle queries with negation\"\n",
    "        else:\n",
    "            # get relation embedding\n",
    "            r_embedding = torch.index_select(model.relation_embedding, dim=0, index=queries[:, idx])\n",
    "            # add entity and relation embedding\n",
    "            embedding += r_embedding\n",
    "        idx += 1\n",
    "else:\n",
    "    embedding_list = []\n",
    "    for i in range(len(query_structure)):\n",
    "        embedding, idx = model.embed_query_vec(queries, query_structure[i], idx)\n",
    "        embedding_list.append(embedding)\n",
    "    embedding = model.center_net(torch.stack(embedding_list))\n",
    "    \n",
    "embedding, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "242dc821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14505, 500]) torch.Size([3, 1, 500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 14505, 500]), torch.Size([3, 14505]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" model.cal_logit_vec \"\"\"\n",
    "# score / calculate distance between candidate embedding and query embedding\n",
    "# negative \n",
    "print(negative_embedding.shape, all_center_embeddings.shape)\n",
    "# model.cal_logit_vec(negative_embedding, all_center_embeddings)\n",
    "\n",
    "#     def cal_logit_vec(self, entity_embedding, query_embedding):\n",
    "#         distance = entity_embedding - query_embedding\n",
    "#         logit = self.gamma - torch.norm(distance, p=1, dim=-1)\n",
    "#         return logit\n",
    "\n",
    "entity_embedding, query_embedding = negative_embedding, all_center_embeddings\n",
    "distance = entity_embedding - query_embedding\n",
    "logit = model.gamma - torch.norm(distance, p=1, dim=-1)\n",
    "\n",
    "distance.shape, logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "814502d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1718, 4586, 8803], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.argmax(axis=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "343fdc80cc68c688baad9e163e1f88e33cc019c87aed1c9258d7ce7a9f5d5e41"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
