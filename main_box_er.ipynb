{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365aa620",
   "metadata": {},
   "source": [
    "based on main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1834111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\r\n",
    "from __future__ import division\r\n",
    "from __future__ import print_function\r\n",
    "\r\n",
    "import argparse\r\n",
    "import json\r\n",
    "import logging\r\n",
    "import os\r\n",
    "import random\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from models import KGReasoning\r\n",
    "from dataloader import TestDataset, TrainDataset, SingledirectionalOneShotIterator\r\n",
    "# from tensorboardX import SummaryWriter\r\n",
    "import time\r\n",
    "import pickle\r\n",
    "from collections import defaultdict\r\n",
    "from tqdm import tqdm\r\n",
    "from util import flatten_query, list2tuple, parse_time, set_global_seed, eval_tuple\r\n",
    "\r\n",
    "import collections\r\n",
    "import random\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "\r\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07eec0",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51cf163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', 'up-DNF', '2u-DM', 'up-DM']\n"
     ]
    }
   ],
   "source": [
    "query_name_dict = {('e',('r',)): '1p', \r\n",
    "                    ('e', ('r', 'r')): '2p',\r\n",
    "                    ('e', ('r', 'r', 'r')): '3p',\r\n",
    "                    (('e', ('r',)), ('e', ('r',))): '2i',\r\n",
    "                    (('e', ('r',)), ('e', ('r',)), ('e', ('r',))): '3i',\r\n",
    "                    ((('e', ('r',)), ('e', ('r',))), ('r',)): 'ip',\r\n",
    "                    (('e', ('r', 'r')), ('e', ('r',))): 'pi',\r\n",
    "                    (('e', ('r',)), ('e', ('r', 'n'))): '2in',\r\n",
    "                    (('e', ('r',)), ('e', ('r',)), ('e', ('r', 'n'))): '3in',\r\n",
    "                    ((('e', ('r',)), ('e', ('r', 'n'))), ('r',)): 'inp',\r\n",
    "                    (('e', ('r', 'r')), ('e', ('r', 'n'))): 'pin',\r\n",
    "                    (('e', ('r', 'r', 'n')), ('e', ('r',))): 'pni',\r\n",
    "                    (('e', ('r',)), ('e', ('r',)), ('u',)): '2u-DNF',\r\n",
    "                    ((('e', ('r',)), ('e', ('r',)), ('u',)), ('r',)): 'up-DNF',\r\n",
    "                    ((('e', ('r', 'n')), ('e', ('r', 'n'))), ('n',)): '2u-DM',\r\n",
    "                    ((('e', ('r', 'n')), ('e', ('r', 'n'))), ('n', 'r')): 'up-DM'\r\n",
    "                }\r\n",
    "name_query_dict = {value: key for key, value in query_name_dict.items()}\r\n",
    "all_tasks = list(name_query_dict.keys()) # ['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', '2u-DM', 'up-DNF', 'up-DM']\r\n",
    "\r\n",
    "print(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb3d2123-3b87-4beb-b401-9188820af9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyArgs:\n",
    "    def __init__(self):\n",
    "        None\n",
    "        \n",
    "args = DummyArgs()\n",
    "args.cuda = True\n",
    "args.geo = \"box\" # choices=['vec', 'box', 'beta']\n",
    "args.gamma = 12.0\n",
    "args.box_mode = \"(none,0.02)\" # Query2box\n",
    "args.beta_mode = \"(1600,2)\" # BetaE relational projection\n",
    "args.data_path = \"data/FB15k-237-betae\"\n",
    "args.batch_size = 64 #1024\n",
    "args.cpu_num = 1# 10\n",
    "args.negative_sample_size = 128\n",
    "args.hidden_dim = 500\n",
    "args.test_batch_size = 3 #2\n",
    "args.print_on_screen = True\n",
    "args.test_log_steps = 1000\n",
    "args.learning_rate = 0.0001\n",
    "args.max_steps = 10000 #100000\n",
    "args.evaluate_union = \"DNF\" # choices=['DNF', 'DM'] evaluate union querioes, disjunctive normal form (DNF) or de Morgan's laws (DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deba0ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', 'up-DNF', '2u-DM', 'up-DM']\n"
     ]
    }
   ],
   "source": [
    "# different query types\r\n",
    "tasks = all_tasks[0:1]\r\n",
    "\r\n",
    "if args.geo in ['box']:\r\n",
    "    tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.box_mode)\r\n",
    "elif args.geo in ['vec']:\r\n",
    "    tmp_str = \"g-{}\".format(args.gamma)\r\n",
    "elif args.geo == 'beta':\r\n",
    "    tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.beta_mode)\r\n",
    "\r\n",
    "print(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25cd5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/stats.txt' % args.data_path) as f:\r\n",
    "    entrel = f.readlines()\r\n",
    "    nentity = int(entrel[0].split(' ')[-1])\r\n",
    "    nrelation = int(entrel[1].split(' ')[-1])\r\n",
    "\r\n",
    "args.nentity = nentity\r\n",
    "args.nrelation = nrelation\r\n",
    "\r\n",
    "logging.info('-------------------------------'*3)\r\n",
    "logging.info('Geo: %s' % args.geo)\r\n",
    "logging.info('Data Path: %s' % args.data_path)\r\n",
    "logging.info('#entity: %d' % nentity)\r\n",
    "logging.info('#relation: %d' % nrelation)\r\n",
    "# logging.info('#max steps: %d' % args.max_steps)\r\n",
    "# logging.info('Evaluate unoins using: %s' % args.evaluate_union)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e2991",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b83ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, tasks):\r\n",
    "    '''\r\n",
    "    Load queries and remove queries not in tasks\r\n",
    "    '''\r\n",
    "    logging.info(\"loading data\")\r\n",
    "    train_queries = pickle.load(open(os.path.join(args.data_path, \"train-queries.pkl\"), 'rb'))\r\n",
    "    train_answers = pickle.load(open(os.path.join(args.data_path, \"train-answers.pkl\"), 'rb'))\r\n",
    "    valid_queries = pickle.load(open(os.path.join(args.data_path, \"valid-queries.pkl\"), 'rb'))\r\n",
    "    valid_hard_answers = pickle.load(open(os.path.join(args.data_path, \"valid-hard-answers.pkl\"), 'rb'))\r\n",
    "    valid_easy_answers = pickle.load(open(os.path.join(args.data_path, \"valid-easy-answers.pkl\"), 'rb'))\r\n",
    "    test_queries = pickle.load(open(os.path.join(args.data_path, \"test-queries.pkl\"), 'rb'))\r\n",
    "    test_hard_answers = pickle.load(open(os.path.join(args.data_path, \"test-hard-answers.pkl\"), 'rb'))\r\n",
    "    test_easy_answers = pickle.load(open(os.path.join(args.data_path, \"test-easy-answers.pkl\"), 'rb'))\r\n",
    "    \r\n",
    "    # remove tasks not in args.tasks\r\n",
    "    for name in all_tasks:\r\n",
    "        if 'u' in name:\r\n",
    "            name, evaluate_union = name.split('-')\r\n",
    "        else:\r\n",
    "            evaluate_union = args.evaluate_union\r\n",
    "        if name not in tasks or evaluate_union != args.evaluate_union:\r\n",
    "            query_structure = name_query_dict[name if 'u' not in name else '-'.join([name, evaluate_union])]\r\n",
    "            if query_structure in train_queries:\r\n",
    "                del train_queries[query_structure]\r\n",
    "            if query_structure in valid_queries:\r\n",
    "                del valid_queries[query_structure]\r\n",
    "            if query_structure in test_queries:\r\n",
    "                del test_queries[query_structure]\r\n",
    "\r\n",
    "    return train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d462c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\r\n",
    "LOAD DATA\r\n",
    "based on tasks defined in args\r\n",
    "\"\"\"\r\n",
    "train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers = load_data(args, tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4fdd4",
   "metadata": {},
   "source": [
    "1. train_queries: for (e, r) given entityID and relationID, find all other entities that are connected to it.\n",
    "2. train_answers: given (e, r) this is the groundtruth answers of all entityIDs that are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd8f7a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(collections.defaultdict, 1),\n",
       " (collections.defaultdict, 1496890),\n",
       " (collections.defaultdict, 1),\n",
       " (collections.defaultdict, 95094),\n",
       " (collections.defaultdict, 95094),\n",
       " (collections.defaultdict, 1),\n",
       " (collections.defaultdict, 97804),\n",
       " (collections.defaultdict, 97804)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(type(x), len(x)) for x in [train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6af0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "(6288, (8,))\n",
      "False\n",
      "valid_hard_answers set()\n",
      "valid_easy_answers set()\n",
      "**********\n",
      "(6288, (8,))\n",
      "True\n",
      "train_answers {9, 11, 117, 399}\n",
      "**********\n",
      "(6288, (8,))\n",
      "False\n",
      "test_hard_answers set()\n",
      "test_easy_answers set()\n"
     ]
    }
   ],
   "source": [
    "ex_val_q = (6288, (8,))\r\n",
    "print(\"*\" * 10)\r\n",
    "print(ex_val_q)\r\n",
    "print(ex_val_q in valid_queries[('e', ('r', ))])\r\n",
    "print(\"valid_hard_answers\", valid_hard_answers[ex_val_q])\r\n",
    "print(\"valid_easy_answers\", valid_easy_answers[ex_val_q])\r\n",
    "\r\n",
    "# get example query\r\n",
    "ex_q = ex_val_q #(6288, (8,))\r\n",
    "print(\"*\" * 10)\r\n",
    "print(ex_q)\r\n",
    "print(ex_q in train_queries[('e', ('r', ))])\r\n",
    "print(\"train_answers\", train_answers[ex_q])\r\n",
    "\r\n",
    "ex_test_q = ex_val_q\r\n",
    "print(\"*\" * 10)\r\n",
    "print(ex_test_q)\r\n",
    "print(ex_test_q in test_queries[('e', ('r', ))])\r\n",
    "print(\"test_hard_answers\", test_hard_answers[ex_val_q])\r\n",
    "print(\"test_easy_answers\", test_easy_answers[ex_val_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e8ae36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train_queries 149689\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "TRAINING Data Preparation\r\n",
    "\"\"\"\r\n",
    "train_path_queries = defaultdict(set)\r\n",
    "for query_structure in train_queries:\r\n",
    "#     train_path_queries[query_structure] = set(random.sample(train_queries[query_structure], 1))\r\n",
    "    train_path_queries[query_structure] = train_queries[query_structure]\r\n",
    "            \r\n",
    "train_path_queries = flatten_query(train_path_queries)\r\n",
    "print(\"number of train_queries\", len(train_path_queries))\r\n",
    "train_path_iterator = SingledirectionalOneShotIterator(DataLoader(\r\n",
    "                            # {(e,r): (entityID, relID)} and {(e, r): [entityID]}\r\n",
    "                            TrainDataset(train_path_queries, nentity, nrelation, args.negative_sample_size, train_answers),\r\n",
    "                            batch_size=args.batch_size,\r\n",
    "                            shuffle=True,\r\n",
    "                            num_workers=args.cpu_num,\r\n",
    "                            collate_fn=TrainDataset.collate_fn\r\n",
    "                        ))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bd504b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error ('e', ('r',))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "VALIDATION Data Preparation\r\n",
    "\"\"\"\r\n",
    "# for each type of structure / key\r\n",
    "for query_structure in valid_queries:\r\n",
    "    try:\r\n",
    "        print(query_name_dict[query_structure[1]]+\": \"+str(len(valid_queries[query_structure])))\r\n",
    "    except:\r\n",
    "        print(\"error\", query_structure)\r\n",
    "\r\n",
    "valid_dataloader = DataLoader(\r\n",
    "    TestDataset(\r\n",
    "        flatten_query(valid_queries), \r\n",
    "        args.nentity, \r\n",
    "        args.nrelation, \r\n",
    "    ), \r\n",
    "    batch_size=args.test_batch_size,\r\n",
    "    num_workers=args.cpu_num, \r\n",
    "    collate_fn=TestDataset.collate_fn\r\n",
    ")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b9c25e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter gamma: torch.Size([1]), require_grad = False\n",
      "Parameter embedding_range: torch.Size([1]), require_grad = False\n",
      "Parameter entity_embedding: torch.Size([14505, 500]), require_grad = True\n",
      "Parameter relation_embedding: torch.Size([474, 500]), require_grad = True\n",
      "Parameter offset_embedding: torch.Size([474, 500]), require_grad = True\n",
      "Parameter center_net.layer1.weight: torch.Size([500, 500]), require_grad = True\n",
      "Parameter center_net.layer1.bias: torch.Size([500]), require_grad = True\n",
      "Parameter center_net.layer2.weight: torch.Size([500, 500]), require_grad = True\n",
      "Parameter center_net.layer2.bias: torch.Size([500]), require_grad = True\n",
      "Parameter offset_net.layer1.weight: torch.Size([500, 500]), require_grad = True\n",
      "Parameter offset_net.layer1.bias: torch.Size([500]), require_grad = True\n",
      "Parameter offset_net.layer2.weight: torch.Size([500, 500]), require_grad = True\n",
      "Parameter offset_net.layer2.bias: torch.Size([500]), require_grad = True\n",
      "Parameter Number: 8728500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "MODEL DEFINITION\r\n",
    "\"\"\"\r\n",
    "model = KGReasoning(\r\n",
    "    nentity=nentity,\r\n",
    "    nrelation=nrelation,\r\n",
    "    hidden_dim=args.hidden_dim,\r\n",
    "    gamma=args.gamma,\r\n",
    "    geo=args.geo,\r\n",
    "    use_cuda = args.cuda,\r\n",
    "    box_mode=eval_tuple(args.box_mode),\r\n",
    "    beta_mode = eval_tuple(args.beta_mode),\r\n",
    "    test_batch_size=args.test_batch_size,\r\n",
    "    query_name_dict = query_name_dict\r\n",
    ")\r\n",
    "\r\n",
    "logging.info('Model Parameter Configuration:')\r\n",
    "num_params = 0\r\n",
    "for name, param in model.named_parameters():\r\n",
    "    print('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\r\n",
    "    if param.requires_grad:\r\n",
    "        num_params += np.prod(param.size())\r\n",
    "print('Parameter Number: %d' % num_params)\r\n",
    "\r\n",
    "model = model.cuda()\r\n",
    "\r\n",
    "current_learning_rate = args.learning_rate\r\n",
    "optimizer = torch.optim.Adam(\r\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \r\n",
    "    lr=current_learning_rate\r\n",
    ")\r\n",
    "warm_up_steps = args.max_steps // 2\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec06c87",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddafb429-8e32-44f0-9ed9-f0b8d6c52131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d961d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: eekosasih (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">2021-09-08T13:57:37.906133</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/eekosasih/KGReasoning\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/eekosasih/KGReasoning/runs/2ww27ppm\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning/runs/2ww27ppm</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210908_145739-2ww27ppm</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2ww27ppm)</h1><iframe src=\"https://wandb.ai/eekosasih/KGReasoning/runs/2ww27ppm\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x21cd803bd48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "timenow = datetime.datetime.utcnow().isoformat()\n",
    "\n",
    "wandb.init(project=\"KGReasoning\", name=timenow, group=\"box\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8effcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "377b36d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9210964a74f4a4080acc81a6118616f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "TRAIN LOOP\r\n",
    "\"\"\"\r\n",
    "init_step = 0\r\n",
    "\r\n",
    "training_logs = []\r\n",
    "# loop over all batches\r\n",
    "# initialising train_path_iterator is expensive. So at certain loop it might take some time to load\r\n",
    "for step in tqdm(range(init_step, args.max_steps)):\r\n",
    "    # train model for a step over all batches\r\n",
    "    log = model.train_step(model, optimizer, train_path_iterator, args, step)\r\n",
    "    training_logs.og, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fde51926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\r\n",
    "torch.save(\r\n",
    "    {\r\n",
    "        'model_state_dict': model.state_dict(),\r\n",
    "        'optimizer_state_dict': optimizer.state_dict()\r\n",
    "    },\r\n",
    "    os.path.join(\"checkpoint\", \"model-box-er.pt\")\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ab898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d9502b1",
   "metadata": {},
   "source": [
    "## DEBUG TRAINING STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f23ad074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive_sample_loss': 0.39944127202033997,\n",
       " 'negative_sample_loss': 1.0847606658935547,\n",
       " 'loss': 0.7421009540557861}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\r\n",
    "\r\n",
    "# model.train_step(model, optimizer, train_other_iterator, args, step)\r\n",
    "\r\n",
    "model, optimizer, train_iterator, args, step = model, optimizer, train_path_iterator, args, step\r\n",
    "\r\n",
    "model.train()\r\n",
    "optimizer.zero_grad()\r\n",
    "\r\n",
    "# there's an overhead operation to shufflte the train_iterator that makes this expensive to run\r\n",
    "positive_sample, negative_sample, subsampling_weight, batch_queries, query_structures = next(train_iterator)\r\n",
    "\r\n",
    "# group queries into batch\r\n",
    "batch_queries_dict = collections.defaultdict(list)\r\n",
    "batch_idxs_dict = collections.defaultdict(list)\r\n",
    "for i, query in enumerate(batch_queries): # group queries with same structure\r\n",
    "    batch_queries_dict[query_structures[i]].append(query)\r\n",
    "    batch_idxs_dict[query_structures[i]].append(i)\r\n",
    "\r\n",
    "for query_structure in batch_queries_dict:\r\n",
    "    if args.cuda:\r\n",
    "        batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure]).cuda()\r\n",
    "    else:\r\n",
    "        batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure])\r\n",
    "\r\n",
    "if args.cuda:\r\n",
    "    positive_sample = positive_sample.cuda()\r\n",
    "    negative_sample = negative_sample.cuda()\r\n",
    "    subsampling_weight = subsampling_weight.cuda()\r\n",
    "\r\n",
    "# score positive and negative samples\r\n",
    "positive_logit, negative_logit, subsampling_weight, _ = model(positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict)\r\n",
    "\r\n",
    "# calculate loss\r\n",
    "# positive samples should have label of 1 while negative samples label of 0\r\n",
    "negative_score = F.logsigmoid(-negative_logit).mean(dim=1)\r\n",
    "positive_score = F.logsigmoid(positive_logit).mean(dim=1)\r\n",
    "# aggregate loss with subsampling_weight\r\n",
    "positive_sample_loss = - (subsampling_weight * positive_score).sum()\r\n",
    "negative_sample_loss = - (subsampling_weight * negative_score).sum()\r\n",
    "positive_sample_loss /= subsampling_weight.sum()\r\n",
    "negative_sample_loss /= subsampling_weight.sum()\r\n",
    "loss = (positive_sample_loss + negative_sample_loss)/2\r\n",
    "\r\n",
    "loss.backward()\r\n",
    "optimizer.step()\r\n",
    "\r\n",
    "log = {\r\n",
    "    'positive_sample_loss': positive_sample_loss.item(),\r\n",
    "    'negative_sample_loss': negative_sample_loss.item(),\r\n",
    "    'loss': loss.item(),\r\n",
    "}\r\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "020b914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== input ===\n",
      "torch.Size([64]) torch.Size([64, 128]) 64 64 torch.Size([64])\n",
      "=== output ===\n",
      "torch.Size([64, 1]) torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== input ===\")\n",
    "print(positive_sample.shape, negative_sample.shape, len(batch_queries), len(query_structures), subsampling_weight.shape)\n",
    "print(\"=== output ===\")\n",
    "print(positive_logit.shape, negative_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfd514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb50afc",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b69abde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model\n",
    "checkpoint = torch.load(os.path.join(\"checkpoint\", \"model-vec-er.pt\"))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e807654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tp_answers, fn_answers, args, dataloader, query_name_dict, mode, step):\n",
    "    '''\n",
    "    Evaluate queries in dataloader\n",
    "    '''\n",
    "    average_metrics = defaultdict(float)\n",
    "    all_metrics = defaultdict(float)\n",
    "\n",
    "    metrics = model.test_step(model, tp_answers, fn_answers, args, dataloader, query_name_dict)\n",
    "    num_query_structures = 0\n",
    "    num_queries = 0\n",
    "    for query_structure in metrics:\n",
    "        log_metrics(mode+\" \"+query_name_dict[query_structure], step, metrics[query_structure])\n",
    "        for metric in metrics[query_structure]:\n",
    "            all_metrics[\"_\".join([query_name_dict[query_structure], metric])] = metrics[query_structure][metric]\n",
    "            if metric != 'num_queries':\n",
    "                average_metrics[metric] += metrics[query_structure][metric]\n",
    "        num_queries += metrics[query_structure]['num_queries']\n",
    "        num_query_structures += 1\n",
    "\n",
    "    for metric in average_metrics:\n",
    "        average_metrics[metric] /= num_query_structures\n",
    "        all_metrics[\"_\".join([\"average\", metric])] = average_metrics[metric]\n",
    "    log_metrics('%s average'%mode, step, average_metrics)\n",
    "\n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "190e62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(mode, step, metrics):\n",
    "    '''\n",
    "    Print the evaluation logs\n",
    "    '''\n",
    "    for metric in metrics:\n",
    "        logging.info('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d980cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"`\n",
    "EVAL\n",
    "\"\"\"\n",
    "step = -1\n",
    "valid_all_metrics = evaluate(model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict, 'Valid', step)\n",
    "valid_all_metrics\n",
    "\n",
    "# after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56911f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5bc0a3",
   "metadata": {},
   "source": [
    "## DEBUG TEST STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a46aade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [02:03<00:00, 54.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function models.KGReasoning.test_step.<locals>.<lambda>()>,\n",
       "            {('e', ('r',)): defaultdict(int,\n",
       "                         {'MRR': 0.009632772394135068,\n",
       "                          'HITS1': 0.008909909141400957,\n",
       "                          'HITS3': 0.00903797208566882,\n",
       "                          'HITS10': 0.009823186617451034,\n",
       "                          'num_queries': 20094})})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def evaluate in main.py\"\"\"\n",
    "\n",
    "metrics = model.test_step(model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6825afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0790d45b52ec4e57820e0dc979625c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "         [    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "         [    0,     1,     2,  ..., 14502, 14503, 14504]]),\n",
       " [[11217, 143], [5673, 34], [4190, 62]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for negative_sample, queries, queries_unflatten, query_structures in tqdm(valid_dataloader, disable=not args.print_on_screen):\n",
    "    break\n",
    "    \n",
    "negative_sample, queries, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeff7c",
   "metadata": {},
   "source": [
    "1. why is there no positive_sample in test_dataloader\n",
    "2. how to embed query vector and perform projection & intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bcfacec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "        [    0,     1,     2,  ..., 14502, 14503, 14504],\n",
       "        [    0,     1,     2,  ..., 14502, 14503, 14504]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afcd0fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32751d7568a470dbbc9a1d898ca9a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {('e', ('r',)): defaultdict(int,\n",
       "                         {'MRR': 0.009632772394135068,\n",
       "                          'HITS1': 0.008909909141400957,\n",
       "                          'HITS3': 0.00903797208566882,\n",
       "                          'HITS10': 0.009823186617451034,\n",
       "                          'num_queries': 20094})})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.test_step in models.py\"\"\"\n",
    "model, easy_answers, hard_answers, args, test_dataloader, query_name_dict = model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict\n",
    "\n",
    "model.eval()\n",
    "\n",
    "step = 0\n",
    "total_steps = len(test_dataloader)\n",
    "logs = collections.defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # process per batch\n",
    "    # loop through every validation dataset\n",
    "    # negative sample is basically all entities in nentity\n",
    "    for negative_sample, queries, queries_unflatten, query_structures in tqdm(test_dataloader, disable=not args.print_on_screen):\n",
    "        batch_queries_dict = collections.defaultdict(list)\n",
    "        batch_idxs_dict = collections.defaultdict(list)\n",
    "        \n",
    "        # for each test query\n",
    "        for i, query in enumerate(queries):\n",
    "            batch_queries_dict[query_structures[i]].append(query)\n",
    "            batch_idxs_dict[query_structures[i]].append(i)\n",
    "            \n",
    "        # convert each positive query to a LongTensor\n",
    "        for query_structure in batch_queries_dict:\n",
    "            if args.cuda:\n",
    "                batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure]).cuda()\n",
    "            else:\n",
    "                batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure])\n",
    "            \n",
    "        # convert each negative query to a LongTensor\n",
    "        if args.cuda:\n",
    "            negative_sample = negative_sample.cuda()\n",
    "            \n",
    "        # get prediction output\n",
    "        # (2, number_of_edges) same dimension with negative_sample\n",
    "        # this is basically embedding lookup\n",
    "        \"\"\"model.forward\"\"\"\n",
    "        # negative_sample here is just a list of all entities, some of them are actually positive\n",
    "        _, negative_logit, _, idxs = model(None, negative_sample, None, batch_queries_dict, batch_idxs_dict)\n",
    "          \n",
    "        # ... scoring ...\n",
    "        queries_unflatten = [queries_unflatten[i] for i in idxs]\n",
    "        query_structures = [query_structures[i] for i in idxs]\n",
    "        # sort from maximum value based on logit values\n",
    "        argsort = torch.argsort(negative_logit, dim=1, descending=True)\n",
    "        ranking = argsort.clone().to(torch.float)\n",
    "        if len(argsort) == args.test_batch_size: # if it is the same shape with test_batch_size, we can reuse batch_entity_range without creating a new one\n",
    "            ranking = ranking.scatter_(1, argsort, model.batch_entity_range) # achieve the ranking of all entities\n",
    "        else: # otherwise, create a new torch Tensor for batch_entity_range\n",
    "            if args.cuda:\n",
    "                ranking = ranking.scatter_(1, argsort, torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0], 1).cuda()) # achieve the ranking of all entities\n",
    "            else:\n",
    "                ranking = ranking.scatter_(1, argsort, torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0], 1)) # achieve the ranking of all entities\n",
    "        \n",
    "        # loop through ranking\n",
    "        # score every query in the dataloader batch\n",
    "        for idx, (i, query, query_structure) in enumerate(zip(argsort[:, 0], queries_unflatten, query_structures)):\n",
    "            # get groundtruth labels\n",
    "            hard_answer = hard_answers[query]\n",
    "            easy_answer = easy_answers[query]\n",
    "            num_hard = len(hard_answer)\n",
    "            num_easy = len(easy_answer)\n",
    "            assert len(hard_answer.intersection(easy_answer)) == 0\n",
    "\n",
    "            # compare ranking of groundtruth (easy_answer, hard_answer) and predicted results\n",
    "            cur_ranking = ranking[idx, list(easy_answer) + list(hard_answer)]\n",
    "            cur_ranking, indices = torch.sort(cur_ranking)\n",
    "            masks = indices >= num_easy\n",
    "            if args.cuda:\n",
    "                answer_list = torch.arange(num_hard + num_easy).to(torch.float).cuda()\n",
    "            else:\n",
    "                answer_list = torch.arange(num_hard + num_easy).to(torch.float)\n",
    "            cur_ranking = cur_ranking - answer_list + 1 # filtered setting\n",
    "            cur_ranking = cur_ranking[masks] # only take indices that belong to the hard answers\n",
    "\n",
    "            mrr = torch.mean(1./cur_ranking).item()\n",
    "            h1 = torch.mean((cur_ranking <= 1).to(torch.float)).item()\n",
    "            h3 = torch.mean((cur_ranking <= 3).to(torch.float)).item()\n",
    "            h10 = torch.mean((cur_ranking <= 10).to(torch.float)).item()\n",
    "\n",
    "            logs[query_structure].append({\n",
    "                'MRR': mrr,\n",
    "                'HITS1': h1,\n",
    "                'HITS3': h3,\n",
    "                'HITS10': h10,\n",
    "                'num_hard_answer': num_hard,\n",
    "            })\n",
    "\n",
    "metrics = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "for query_structure in logs:\n",
    "    for metric in logs[query_structure][0].keys():\n",
    "        if metric in ['num_hard_answer']:\n",
    "            continue\n",
    "        metrics[query_structure][metric] = sum([log[metric] for log in logs[query_structure]])/len(logs[query_structure])\n",
    "    metrics[query_structure]['num_queries'] = len(logs[query_structure])\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4caf10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[6.2495, 6.0508, 6.2885,  ..., 6.2162, 6.1718, 5.5565],\n",
       "         [5.2960, 5.4308, 5.9617,  ..., 5.8868, 5.7818, 5.8520],\n",
       "         [5.7562, 5.4614, 6.0019,  ..., 5.8199, 5.7874, 5.8295]],\n",
       "        device='cuda:0', grad_fn=<CatBackward>),\n",
       " None,\n",
       " [0, 1, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.forward\"\"\"\n",
    "# model.forward(self, positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict)\n",
    "\n",
    "# model.geo == \"box\"\n",
    "\n",
    "# done per batch\n",
    "\"\"\"model.forward_box\"\"\"\n",
    "# compared to vec, just add union of offset embeddings\n",
    "\n",
    "# model.forward_box(self, positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict)\n",
    "# negative_sample is a list of entities to be scored as potential answers to the queries\n",
    "# batch_queries_dict stores list of queries (in this case (e, r) to be scored)\n",
    "positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict = None, negative_sample, None, batch_queries_dict, batch_idxs_dict\n",
    "\n",
    "all_center_embeddings, all_offset_embeddings, all_idxs = [], [], []\n",
    "all_union_center_embeddings, all_union_offset_embeddings, all_union_idxs = [], [], []\n",
    "for query_structure in batch_queries_dict:\n",
    "    if 'u' in model.query_name_dict[query_structure]:\n",
    "        # embedding \n",
    "        center_embedding, offset_embedding, _ = model.embed_query_vbox(model.transform_union_query(batch_queries_dict[query_structure], \n",
    "                                                            query_structure), \n",
    "                                                        model.transform_union_structure(query_structure), 0)\n",
    "        all_union_center_embeddings.append(center_embedding)\n",
    "        all_union_offset_embeddings.append(offset_embedding)\n",
    "        all_union_idxs.extend(batch_idxs_dict[query_structure])\n",
    "    else:\n",
    "        # get vector embedding for each query (anchor + projection + intersection)\n",
    "        # this will be compared with the groundtruth answer entity embedding\n",
    "        center_embedding, offset_embedding, _ = model.embed_query_box(batch_queries_dict[query_structure], query_structure, 0)\n",
    "        all_center_embeddings.append(center_embedding)\n",
    "        all_offset_embeddings.append(offset_embedding)\n",
    "        all_idxs.extend(batch_idxs_dict[query_structure])\n",
    "\n",
    "if len(all_center_embeddings) > 0 and len(all_offset_embeddings) > 0:\n",
    "    all_center_embeddings = torch.cat(all_center_embeddings, dim=0).unsqueeze(1)\n",
    "    all_offset_embeddings = torch.cat(all_offset_embeddings, dim=0).unsqueeze(1)\n",
    "if len(all_union_center_embeddings) > 0 and len(all_union_offset_embeddings) > 0:\n",
    "    all_union_center_embeddings = torch.cat(all_union_center_embeddings, dim=0).unsqueeze(1)\n",
    "    all_union_offset_embeddings = torch.cat(all_union_offset_embeddings, dim=0).unsqueeze(1)\n",
    "    all_union_center_embeddings = all_union_center_embeddings.view(all_union_center_embeddings.shape[0]//2, 2, 1, -1)\n",
    "    all_union_offset_embeddings = all_union_offset_embeddings.view(all_union_offset_embeddings.shape[0]//2, 2, 1, -1)\n",
    "\n",
    "if type(subsampling_weight) != type(None):\n",
    "    subsampling_weight = subsampling_weight[all_idxs+all_union_idxs]\n",
    "\n",
    "# in test, positive samples are not given\n",
    "if type(positive_sample) != type(None):\n",
    "    if len(all_center_embeddings) > 0:\n",
    "        positive_sample_regular = positive_sample[all_idxs]\n",
    "        positive_embedding = torch.index_select(model.entity_embedding, dim=0, index=positive_sample_regular).unsqueeze(1)\n",
    "        positive_logit = model.cal_logit_box(positive_embedding, all_center_embeddings, all_offset_embeddings)\n",
    "    else:\n",
    "        positive_logit = torch.Tensor([]).to(model.entity_embedding.device)\n",
    "\n",
    "    if len(all_union_center_embeddings) > 0:\n",
    "        positive_sample_union = positive_sample[all_union_idxs]\n",
    "        positive_embedding = torch.index_select(model.entity_embedding, dim=0, index=positive_sample_union).unsqueeze(1).unsqueeze(1)\n",
    "        positive_union_logit = model.cal_logit_box(positive_embedding, all_union_center_embeddings, all_offset_embeddings)\n",
    "        positive_union_logit = torch.max(positive_union_logit, dim=1)[0]\n",
    "    else:\n",
    "        positive_union_logit = torch.Tensor([]).to(model.entity_embedding.device)\n",
    "    positive_logit = torch.cat([positive_logit, positive_union_logit], dim=0)\n",
    "else:\n",
    "    positive_logit = None\n",
    "\n",
    "# in test, negative samples are basically all possible list of entities\n",
    "if type(negative_sample) != type(None):\n",
    "    if len(all_center_embeddings) > 0:\n",
    "        negative_sample_regular = negative_sample[all_idxs]\n",
    "        batch_size, negative_size = negative_sample_regular.shape\n",
    "        negative_embedding = torch.index_select(model.entity_embedding, dim=0, index=negative_sample_regular.view(-1)).view(batch_size, negative_size, -1)\n",
    "        negative_logit = model.cal_logit_box(negative_embedding, all_center_embeddings, all_offset_embeddings)\n",
    "    else:\n",
    "        negative_logit = torch.Tensor([]).to(model.entity_embedding.device)\n",
    "\n",
    "    if len(all_union_center_embeddings) > 0:\n",
    "        negative_sample_union = negative_sample[all_union_idxs]\n",
    "        batch_size, negative_size = negative_sample_union.shape\n",
    "        negative_embedding = torch.index_select(model.entity_embedding, dim=0, index=negative_sample_union.view(-1)).view(batch_size, 1, negative_size, -1)\n",
    "        negative_union_logit = model.cal_logit_(neboxgative_embedding, all_union_center_embeddings, all_offset_embeddings)\n",
    "        negative_union_logit = torch.max(negative_union_logit, dim=1)[0]\n",
    "    else:\n",
    "        negative_union_logit = torch.Tensor([]).to(model.entity_embedding.device)\n",
    "    negative_logit = torch.cat([negative_logit, negative_union_logit], dim=0)\n",
    "else:\n",
    "    negative_logit = None\n",
    "\n",
    "positive_logit, negative_logit, subsampling_weight, all_idxs+all_union_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7ac8eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0481, -0.0327,  0.0139,  ...,  0.0114, -0.0135, -0.0339],\n",
       "         [ 0.0293, -0.0434, -0.0025,  ...,  0.0150,  0.0325,  0.0024],\n",
       "         [-0.0111, -0.0158, -0.0044,  ..., -0.0344,  0.0276, -0.0045]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[0.0392, 0.0410, 0.0311,  ..., 0.0323, 0.0458, 0.0522],\n",
       "         [0.0257, 0.0153, 0.0160,  ..., 0.0154, 0.0040, 0.0260],\n",
       "         [0.0090, 0.0043, 0.0554,  ..., 0.0342, 0.0494, 0.0101]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.embed_query_box in models.py\"\"\"\n",
    "# get the query embedding\n",
    "# input is (batch_size, query_type_length=2), query_structure=(e, r)\n",
    "# model.embed_query_vec(batch_queries_dict[query_structure], query_structure, 0)\n",
    "\n",
    "queries, query_structure, idx = batch_queries_dict[query_structure], query_structure, 0\n",
    "\n",
    "all_relation_flag = True\n",
    "for ele in query_structure[-1]:\n",
    "    if ele not in ['r', 'n']:\n",
    "        all_relation_flag = False\n",
    "        break\n",
    "if all_relation_flag:\n",
    "    # if anchor\n",
    "    if query_structure[0] == \"e\":\n",
    "        # get entity embedding\n",
    "        embedding = torch.index_select(model.entity_embedding, dim=0, index=queries[:, idx])\n",
    "        idx += 1\n",
    "    else:\n",
    "        # recursion\n",
    "        embedding, offset_embedding, idx = model.embed_query_box(queries, query_structure[0], idx)\n",
    "    for i in range(len(query_structure[-1])):\n",
    "        if query_structure[-1][i] == \"n\":\n",
    "            assert False, \"box cannot handle queries with negation\"\n",
    "        else:\n",
    "            # get relation embedding\n",
    "            r_embedding = torch.index_select(model.relation_embedding, dim=0, index=queries[:, idx])\n",
    "            # get offset embedding\n",
    "            r_offset_embedding = torch.index_select(model.offset_embedding, dim=0, index=queries[:, idx])\n",
    "            # add entity and relation embedding\n",
    "            embedding += r_embedding\n",
    "            offset_embedding += model.func(r_offset_embedding)\n",
    "        idx += 1\n",
    "else:\n",
    "    embedding_list = []\n",
    "    offset_embedding_list = []\n",
    "    for i in range(len(query_structure)):\n",
    "        embedding, offset_embedding, idx = model.embed_query_box(queries, query_structure[i], idx)\n",
    "        embedding_list.append(embedding)\n",
    "        offset_embedding_list.append(offset_embedding)\n",
    "    embedding = model.center_net(torch.stack(embedding_list))\n",
    "    offset_embedding = model.offset_net(torch.stack(offset_embedding_list))\n",
    "    \n",
    "embedding, offset_embedding, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "693c9903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14505, 500]) torch.Size([3, 1, 500]) torch.Size([3, 1, 500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 14505, 500]),\n",
       " torch.Size([3, 14505, 500]),\n",
       " torch.Size([3, 14505, 500]),\n",
       " torch.Size([3, 14505]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" model.cal_logit_box \"\"\"\n",
    "# score / calculate distance between candidate embedding and query embedding\n",
    "# negative \n",
    "print(negative_embedding.shape, all_center_embeddings.shape, all_offset_embeddings.shape)\n",
    "\n",
    "entity_embedding, query_center_embedding, query_offset_embedding = negative_embedding, all_center_embeddings, all_offset_embeddings\n",
    "\n",
    "delta = (entity_embedding - query_center_embedding).abs()\n",
    "distance_out = F.relu(delta - query_offset_embedding)\n",
    "distance_in = torch.min(delta, query_offset_embedding)\n",
    "logit = model.gamma - torch.norm(distance_out, p=1, dim=-1) - model.cen * torch.norm(distance_in, p=1, dim=-1)\n",
    "\n",
    "delta.shape, distance_out.shape, distance_in.shape, logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8720ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94894c0a-a032-4ad1-8a80-13a9fd53a3f3",
   "metadata": {},
   "source": [
    "## Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d68d793d-29e2-47d7-a030-d499bdd0df7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19980<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210908_150011-14331z5a\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210908_150011-14331z5a\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>positive_sample_loss</td><td>0.00287</td></tr><tr><td>negative_sample_loss</td><td>5.85275</td></tr><tr><td>loss</td><td>2.92781</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>_runtime</td><td>65</td></tr><tr><td>_timestamp</td><td>1631109676</td></tr><tr><td>_step</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>positive_sample_loss</td><td>▁</td></tr><tr><td>negative_sample_loss</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">2021-09-08T14:00:11.269012</strong>: <a href=\"https://wandb.ai/eekosasih/KGReasoning/runs/14331z5a\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning/runs/14331z5a</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feb347ab-7edb-4fbb-98b6-27989304f902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: eekosasih (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">2021-09-08T15:35:55.168873</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/eekosasih/KGReasoning\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/eekosasih/KGReasoning/runs/6dnbd1mx\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning/runs/6dnbd1mx</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210908_163557-6dnbd1mx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(6dnbd1mx)</h1><iframe src=\"https://wandb.ai/eekosasih/KGReasoning/runs/6dnbd1mx\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1c7afcc4588>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timenow = datetime.datetime.utcnow().isoformat()\n",
    "wandb.init(project=\"KGReasoning\", name=timenow, group=\"box\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5dd3fdd-de39-406a-aca0-a660fe733042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:01<00:00, 55.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:08<00:00, 52.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:07<00:00, 52.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:09<00:00, 51.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:06<00:00, 53.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:07<00:00, 52.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:08<00:00, 52.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:06<00:00, 53.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:07<00:00, 52.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:08<00:00, 51.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:06<00:00, 52.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:07<00:00, 52.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:07<00:00, 52.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:09<00:00, 51.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:11<00:00, 51.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.20it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:01<00:00, 54.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:01<00:00, 55.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:07<00:00, 52.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:01<00:00, 55.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:01<00:00, 55.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:03<00:00, 54.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:02<00:00, 54.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:04<00:00, 53.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:07<00:00, 52.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:06<00:00, 53.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.20it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:07<00:00, 52.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:09<00:00, 51.90it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:06<00:00, 53.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:06<00:00, 53.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:10<00:00, 51.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:06<00:00, 52.90it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:05<00:00, 53.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6698/6698 [02:06<00:00, 53.09it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TRAIN AND EVALUATE\n",
    "\"\"\"\n",
    "init_step = 0\n",
    "\n",
    "training_logs = []\n",
    "# loop over all batches\n",
    "# initialising train_path_iterator is expensive. So at certain loop it might take some time to load\n",
    "# for step in tqdm(range(init_step, args.max_steps)):\n",
    "for step in range(init_step, args.max_steps):\n",
    "    # train model for a step over all batches\n",
    "    log = model.train_step(model, optimizer, train_path_iterator, args, step)\n",
    "    training_logs.append(log)\n",
    "\n",
    "    log[\"epoch\"] = step\n",
    "    wandb.log(log, step=step)\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        valid_all_metrics = evaluate(model, valid_easy_answers, valid_hard_answers, args, valid_dataloader, query_name_dict, 'Valid', step)\n",
    "        wandb.log(valid_all_metrics, step=step)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bcadfa2-e17b-4316-bd80-1be783c2b469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16352<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210908_163557-6dnbd1mx\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\eek31\\Documents\\GitHub\\KGReasoning\\wandb\\run-20210908_163557-6dnbd1mx\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>positive_sample_loss</td><td>0.07303</td></tr><tr><td>negative_sample_loss</td><td>0.05302</td></tr><tr><td>loss</td><td>0.06303</td></tr><tr><td>epoch</td><td>9999</td></tr><tr><td>1p_MRR</td><td>0.27395</td></tr><tr><td>1p_HITS1</td><td>0.19752</td></tr><tr><td>1p_HITS3</td><td>0.29766</td></tr><tr><td>1p_HITS10</td><td>0.43161</td></tr><tr><td>1p_num_queries</td><td>20094</td></tr><tr><td>average_MRR</td><td>0.27395</td></tr><tr><td>average_HITS1</td><td>0.19752</td></tr><tr><td>average_HITS3</td><td>0.29766</td></tr><tr><td>average_HITS10</td><td>0.43161</td></tr><tr><td>_runtime</td><td>18628</td></tr><tr><td>_timestamp</td><td>1631133985</td></tr><tr><td>_step</td><td>9999</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>positive_sample_loss</td><td>▄▆▇▆█▅▇▄▅▅▄▄▄▃▅▃▃▃▄▂▂▃▂▂▃▂▂▂▂▁▂▁▁▂▂▂▁▁▁▁</td></tr><tr><td>negative_sample_loss</td><td>█▅▃▃▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▆▅▄▅▄▅▃▄▃▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>1p_MRR</td><td>▁▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>1p_HITS1</td><td>▁▂▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>1p_HITS3</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>1p_HITS10</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>1p_num_queries</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>average_MRR</td><td>▁▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>average_HITS1</td><td>▁▂▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>average_HITS3</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>average_HITS10</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">2021-09-08T15:35:55.168873</strong>: <a href=\"https://wandb.ai/eekosasih/KGReasoning/runs/6dnbd1mx\" target=\"_blank\">https://wandb.ai/eekosasih/KGReasoning/runs/6dnbd1mx</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2ecfb-cb99-46c5-929d-f30920269248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "343fdc80cc68c688baad9e163e1f88e33cc019c87aed1c9258d7ce7a9f5d5e41"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
